{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modularize/get_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modularize/get_data.py\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "#setup path to data folder\n",
    "data_path = Path('data/')\n",
    "image_path = data_path/'pizza_steak_sushi'\n",
    "\n",
    "#if the iumage folder doesn't exist, download and prepare the data\n",
    "if image_path.is_dir():\n",
    "    print(f'{image_path} already exists')\n",
    "else:\n",
    "    print(f'{image_path} does not exist, downloading files...')\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#download the data to the image_path\n",
    "with open('data/pizza_steak_sushi.zip', 'wb') as f:\n",
    "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
    "    print(\"Downloading pizza, steak, sushi data...\")\n",
    "    f.write(request.content)\n",
    "\n",
    "#unzip the data\n",
    "with zipfile.ZipFile('data/pizza_steak_sushi.zip', 'r') as zip_ref:\n",
    "    zip_ref.extractall(image_path)\n",
    "    print(\"Data unzipped!\")\n",
    "\n",
    "#remove the zip file\n",
    "os.remove(data_path/'pizza_steak_sushi.zip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modularize/data_setup.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modularize/data_setup.py\n",
    "\"\"\"\n",
    "Contains functionality for creating PyTorch DataLoaders for \n",
    "image classification data.\n",
    "\"\"\"\n",
    "import os\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "    train_dir: str, \n",
    "    test_dir: str, \n",
    "    transform: transforms.Compose, \n",
    "    batch_size: int, \n",
    "    num_workers: int=NUM_WORKERS\n",
    "):\n",
    "  \"\"\"Creates training and testing DataLoaders.\n",
    "\n",
    "  Takes in a training directory and testing directory path and turns\n",
    "  them into PyTorch Datasets and then into PyTorch DataLoaders.\n",
    "\n",
    "  Args:\n",
    "    train_dir: Path to training directory.\n",
    "    test_dir: Path to testing directory.\n",
    "    transform: torchvision transforms to perform on training and testing data.\n",
    "    batch_size: Number of samples per batch in each of the DataLoaders.\n",
    "    num_workers: An integer for number of workers per DataLoader.\n",
    "\n",
    "  Returns:\n",
    "    A tuple of (train_dataloader, test_dataloader, class_names).\n",
    "    Where class_names is a list of the target classes.\n",
    "    Example usage:\n",
    "      train_dataloader, test_dataloader, class_names = \\\n",
    "        = create_dataloaders(train_dir=path/to/train_dir,\n",
    "                            test_dir=path/to/test_dir,\n",
    "                            transform=some_transform,\n",
    "                            batch_size=32,\n",
    "                            num_workers=4)\n",
    "  \"\"\"\n",
    "  # Use ImageFolder to create dataset(s)\n",
    "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "  # Get class names\n",
    "  class_names = train_data.classes\n",
    "\n",
    "  # Turn images into data loaders\n",
    "  train_dataloader = DataLoader(\n",
    "      train_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "  test_dataloader = DataLoader(\n",
    "      test_data,\n",
    "      batch_size=batch_size,\n",
    "      shuffle=True,\n",
    "      num_workers=num_workers,\n",
    "      pin_memory=True,\n",
    "  )\n",
    "\n",
    "  return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modularize/model_builder.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modularize/model_builder.py\n",
    "\"\"\"\n",
    "Contains PyTorch model code to instantiate a TinyVGG model.\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class TinyVGG(nn.Module):\n",
    "    \"\"\"Creates the TinyVGG architecture.    \n",
    "    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
    "    See the original architecture here: https://poloclub.github.io/cnn-explainer/   \n",
    "    Args:\n",
    "        input_shape: An integer indicating number of input channels.\n",
    "        hidden_units: An integer indicating number of hidden units between layers.\n",
    "        output_shape: An integer indicating number of output units.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, \n",
    "                        out_channels=hidden_units, \n",
    "                        kernel_size=3, \n",
    "                        stride=1, \n",
    "                        padding=0),  \n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, \n",
    "                        out_channels=hidden_units,\n",
    "                        kernel_size=3,\n",
    "                        stride=1,\n",
    "                        padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                            stride=2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            #each layer of our network compresses and changes the shape of our inputs data.\n",
    "            nn.Linear(in_features=hidden_units*13*13,\n",
    "                        out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "        #return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting modularize/engine.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modularize/engine.py\n",
    "\"\"\"\n",
    "Contains functionality for training and testing a pytorch model\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "def train_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"trains a model for a single epoch\n",
    "\n",
    "    turns a target model to training mode and then runs through\n",
    "    all of the required training steps (forward pass, loss calculation, optimizer step)\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model to train.\n",
    "        dataloader: A PyTorch DataLoader with training data.\n",
    "        loss_fn: A PyTorch loss function.\n",
    "        optimizer: A PyTorch optimizer.\n",
    "        device: A PyTorch device.\n",
    "\n",
    "\n",
    "    returns:\n",
    "        A tuple of (loss, accuracy) for the epoch.\n",
    "        in the form (train_loss, train_accuracy) for example:\n",
    "        (1.2345, 0.5678)\n",
    "    \"\"\"\n",
    "    #set model to training mode\n",
    "    model.train()\n",
    "\n",
    "    #set train loss and train accuracy to 0\n",
    "    train_loss = 0, train_acc = 0\n",
    "\n",
    "    #loop through data loader data batches\n",
    "    for batch, (X,y) in enumerate(dataloader):\n",
    "        #send data to device\n",
    "        X,y = X.to(device), y.to(device)\n",
    "        #forward pass\n",
    "        preds = model(X)\n",
    "        #calculate loss\n",
    "        loss = loss_fn(preds, y)\n",
    "        train_loss += loss.item()\n",
    "        #optimzer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        #backward pass\n",
    "        loss.backward()\n",
    "        #optimizer step\n",
    "        optimizer.step()\n",
    "        #calculate accuracy\n",
    "        pred_class = torch.argmax(sofmax(preds, dim=1), dim=1)\n",
    "        train_acc += (pred_class == y).sum().item() / len(preds)\n",
    "\n",
    "    #adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def test_step(\n",
    "    model: torch.nn.Module,\n",
    "    dataloader: torch.utils.data.DataLoader,\n",
    "    loss_fn: torch.nn.Module,\n",
    "    device: torch.device,\n",
    ") -> Tuple[float, float]:\n",
    "    \"\"\"tests a model for a single epoch\n",
    "\n",
    "    Turns a target model to evaluation mode and then runs through\n",
    "    a forward pass on a testing dataset\n",
    "\n",
    "    Args:\n",
    "        model: A PyTorch model to test.\n",
    "        dataloader: A PyTorch DataLoader with testing data.\n",
    "        loss_fn: A PyTorch loss function.\n",
    "        device: A PyTorch device.\n",
    "\n",
    "    returns:\n",
    "        A tuple of (loss, accuracy) for the epoch.\n",
    "        in the form (test_loss, test_accuracy) for example:\n",
    "        (1.2345, 0.5678)\n",
    "    \"\"\"\n",
    "    #set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    #set test loss and test accuracy to 0\n",
    "    test_loss = 0, test_acc = 0\n",
    "\n",
    "    #turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        #loop through data loader data batches\n",
    "        for batch, (X,y) in enumerate(dataloader):\n",
    "            #send data to device\n",
    "            X,y = X.to(device), y.to(device)\n",
    "            #forward pass\n",
    "            preds = model(X)\n",
    "            #calculate loss\n",
    "            loss = loss_fn(preds, y)\n",
    "            test_loss += loss.item()\n",
    "            #calculate accuracy\n",
    "            pred_class = torch.argmax(sofmax(preds, dim=1), dim=1)\n",
    "            test_acc += (pred_class == y).sum().item() / len(preds)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modularize/utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modularize/utils.py\n",
    "\"\"\"\n",
    "contains a bunch a utility functions for training and saving\n",
    "\"\"\"\n",
    "import torch\n",
    "from pathlib import Path\n",
    "def save_model(model: torch.nn.Module, target_dir: str, model_name: str):\n",
    "    \"\"\"saves a model to a target directory\n",
    "    Args:\n",
    "        model: a pytorch model to save\n",
    "        target_dir: a directory for saving the model to\n",
    "        model_name: a filename for the saved model sould include either .pth or .pt as extension.\n",
    "    \n",
    "    example:\n",
    "        save_model(model_0, models, tinyvgg_model.pth)\n",
    "    \"\"\"\n",
    "    #create target directory\n",
    "    target_dir_path = Path(target_dir)\n",
    "    target_dir_path.mkdir(True, True)\n",
    "\n",
    "    #create model save path\n",
    "    assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"wrong extension, pth or pt pls\"\n",
    "    model_save_path = target_dir_path / model_name\n",
    "\n",
    "    #save state dict\n",
    "    print(f'[INFO] saving model to: {model_save_path}')\n",
    "    torch.save(model.state_dict, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing modularize/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile modularize/train.py\n",
    "\"\"\"\n",
    "trains a pytorch image classification model using device-agnostic code.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import data_setup, engine, model_builder, utils\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "# Setup hyperparameters\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_UNITS = 10\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Setup directories\n",
    "train_dir = \"data/pizza_steak_sushi/train\"\n",
    "test_dir = \"data/pizza_steak_sushi/test\"\n",
    "\n",
    "# Setup target device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Create transforms\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Create DataLoaders with help from data_setup.py\n",
    "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=data_transform,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Create model with help from model_builder.py\n",
    "model = model_builder.TinyVGG(\n",
    "    input_shape=3,\n",
    "    hidden_units=HIDDEN_UNITS,\n",
    "    output_shape=len(class_names)\n",
    ").to(device)\n",
    "\n",
    "# Set loss and optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                            lr=LEARNING_RATE)\n",
    "\n",
    "# Start training with help from engine.py\n",
    "engine.train(model=model,\n",
    "            train_dataloader=train_dataloader,\n",
    "            test_dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optimizer,\n",
    "            epochs=NUM_EPOCHS,\n",
    "            device=device)\n",
    "\n",
    "# Save the model with help from utils.py\n",
    "utils.save_model(model=model,\n",
    "                target_dir=\"models\",\n",
    "                model_name=\"tinyvgg_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
