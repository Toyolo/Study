{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the following is the first notebook entry as i start to learn machine learning myself\n",
    "## hopefully i can learn to do this and keep up a journal of my progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scalar\n",
    "scalar = torch.tensor(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scalars are 0 dimensional tensors, they have no shape, no length, no dimension, no size\n",
    "they are created the same way as tensors, but with a single number, not a list\n",
    "the dimensions of a tensor a tensor start at 0 and then go to 1, 2, 3, etc \n",
    "with a one dimensional tensor, you can think of it as a list of numbers like [1, 2, 3, 4, 5]\n",
    "for example torch.tensor([1, 2, 3, 4, 5]) is a one dimensional tensor with 5 elements\n",
    "and torch.tensor([1, 2, 3, 4, 5]).shape is torch.Size([5])\n",
    "whereas torch.tensor(7) is a 0 dimensional tensor with 1 element\n",
    "and torch.tensor([1],[2]) is a 2 dimensional tensor with 2 elements\n",
    "and torch.tensor([1],[2]).shape is torch.Size([2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar dimensions:  0\n"
     ]
    }
   ],
   "source": [
    "#to check the dimensions of a tensor (number of rows and columns) ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "scalar.ndim \n",
    "#for clarity                                                    #         ⣀⣀⠀⠀⠀⠀⠀⠀⠀ ⠀⣀⣀⠀\n",
    "print(\"scalar dimensions: \", scalar.ndim)                       #⠀⠀⠀⢀⣠⣶⣿⣿⣿⣄⣀⣀⣀⣀⣀⣀⣠⣿⣿⣿⣶⣄⡀⠀⠀⠀@toyolo on discord\n",
    "                                                                #⠀⠀⢀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡀⠀⠀\n",
    "                                                                #⠀⠀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⠀⠀\n",
    "                                                                #⠀⢰⣿⣿⣿⣿⣿⡿⠋⠉⠻⣿⣿⣿⣿⠟⠉⠙⢿⣿⣿⣿⣿⣿⡆⠀\n",
    "                                                                #⠀⣿⣿⣿⣿⣿⣿ ⠀⠀⠀ ⣿⣿⣿⣿⠀⠀⠀ ⢸⣿⣿⣿⣿⣿⠀\n",
    "                                                                #⢀⣿⣿⣿⣿⣿⣿⣿⣄⣀⣴⣿⣿⣿⣿⣦⣀⣠⣾⣿⣿⣿⣿⣿⣿⡀\n",
    "                                                                #⢸⣿⣿⣿⣿⡿⠛⠛⠻⠿⢿⣿⣿⣿⣿⡿⠿⠛⠛⠛⢿⣿⣿⣿⣿⡇\n",
    "                                                                #⠈⢿⣿⣿⣿⣧⣄⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣀⣠⣼⣿⣿⣿⡿⠁\n",
    "                                                                #⠀⠀⠉⠛⠿⠿⣿⣿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠙⣿⣿⠿⠿⠛⠉⠀⠀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get the number from the tensor\n",
    "scalar.item()\n",
    "#this will convert the value inside of the tensor to a python integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a vector is a single dimensional tensor that can have as many values as you want\n",
    "vector = torch.tensor([7,7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the dimensions of the vector\n",
    "vector.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the number of dimensions will match the number of brackets in the array\n",
    "([]) = 1D array (vector), ([[]]) = 2D array, ([[[ ]]]) = 3D array\n",
    "the shape of a tensor is how the elements inside the tensor is arranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the tensor is:  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "#this is how you check the shape\n",
    "print(\"The shape of the tensor is: \", vector.shape)\n",
    "#the size([2])means our vector has a shape of [2] this is because of the 2 elements in the vector\n",
    "#([7,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#this is a matrix\n",
    "matrix = torch.tensor([[7,8],\n",
    "                      [9,10]])\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.ndim\n",
    "#its a 2D Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape\n",
    "#the shape is 2 rows and 2 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up until now they have all been gone over as tensors\n",
    "but its really like\n",
    "0D tensor = scalar, 1D tensor = vector, 2D tensor = matrix\n",
    "3D tensor = cube of numbers\n",
    "anything else would be a higher order tensor (4D, 5D, etc)\n",
    "like a hypercube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [3, 6, 9],\n",
       "         [2, 4, 5]]])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is a tensor\n",
    "TENSOR = torch.tensor([[[1,2,3],\n",
    "                        [3,6,9],\n",
    "                        [2,4,5]]])\n",
    "TENSOR\n",
    "#tensors can represent almost anything\n",
    "#the above tensor is a representation of sales numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TENSOR.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.shape\n",
    "#this is a bit confusing as it outputs ([1,3,3])\n",
    "#but its a 3 dimenrsional tensor\n",
    "#the dimensions go outter to inner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nin conclusion, i have learned about the following:\\nscalar = single number and its dimension is 0\\nvector = a number with dirextion(like wind speed with direction) its dimension is 1\\nmatrix = a 2d array of numbers its dimension is 2\\ntensor = a n-d array of numbers its dimension is n\\n'"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "in conclusion, i have learned about the following:\n",
    "scalar = single number and its dimension is 0\n",
    "vector = a number with dirextion(like wind speed with direction) its dimension is 1\n",
    "matrix = a 2d array of numbers its dimension is 2\n",
    "tensor = a n-d array of numbers its dimension is n\n",
    "\"\"\"\n",
    "#ive learned i can use docstrings randomly for notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7220, 0.3320, 0.5841, 0.1807],\n",
       "         [0.2211, 0.7091, 0.9269, 0.1750],\n",
       "         [0.6294, 0.7131, 0.0221, 0.7419]]),\n",
       " torch.float32,\n",
       " 2,\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to create a tensor of random numbers\n",
    "random_tensor = torch.rand(size=(3,4))\n",
    "random_tensor, random_tensor.dtype, random_tensor.ndim, random_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([244, 244, 3]), 3)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this can be used to make random tensors in whatever size you want\n",
    "#for example you could use torch.random(244,244,3)\n",
    "#this wuold follow the common shape of an image\n",
    "random_image_size_tensor = torch.rand(size=(244,244,3))\n",
    "random_image_size_tensor.shape, random_image_size_tensor.ndim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hey copilot do i need to specify the size = key?\n",
    "- copilot: no, you can just do torch.rand((244,244,3))\n",
    "- why the double parenthesis?\n",
    "- copilot: because it's a tuple\n",
    "- so rand takes a tuple as an argument?\n",
    "- copilot: yes\n",
    "- will rand(244,244,3) work?\n",
    "- copilot: no, it will throw an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to create a tensor full of zeros\n",
    "zeros = torch.zeros(3,4)\n",
    "zeros, zeros.dtype\n",
    "#this can be used to create a tensor of any shape\n",
    "#and the zeros can be accessed and filled with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to create a tensor full of ones\n",
    "ones = torch.ones(3,4)\n",
    "ones, ones.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to create a range of values\n",
    "zero_to_ten = torch.arange(start=0, end=10, step=1)\n",
    "\n",
    "zero_to_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the pytorch arange (not arrange) function\n",
    "it is used to create a range of values with a set interval\n",
    "the start is the first value in the range\n",
    "the end is the last value in the range\n",
    "the step is the interval between each value in the range\n",
    "its important to remember it returns a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hey copilot is like everything in torch just about tensors?\n",
    "- copilot: yes, tensors are the main data structure in torch\n",
    "- copilot: tensors are like numpy arrays, but they can also be used on a GPU to accelerate computing\n",
    "- copilot: tensors are also optimized for automatic differentiation\n",
    "- copilot: tensors are also the main data structure for neural networks in pytorch\n",
    "- thanks copilot\n",
    "- copilot: no problem\n",
    "- copilot: I'm here to help\n",
    "- is shape a very important thing in torch?\n",
    "- copilot: yes, shape is very important in torch\n",
    "- so in the same way that i would need torch.zeros and torch.ones\n",
    "to create a tensor of zeros and ones of any size from the jump\n",
    "i would need torch.zeros_like and torch.ones_like\n",
    "to create a tensor of ones or zeros in the same shape\n",
    "as some other data im working with?\n",
    "- copilot: yes, that's correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#to create a tensor of zeros or ones in the same shape as some other data\n",
    "ten_zeros = torch.zeros_like(zero_to_ten)\n",
    "ten_zeros "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- copilot why are there so many data types in pytorch?\n",
    "- copilot: because it's a dynamic language\n",
    "- copilot: and it's a language that's designed to be used in a dynamic way\n",
    "- is it bc some of the data types are for the gpu and some are for the cpu\n",
    "- copilot: no, it's because some of the data types are for the gpu and some are for the cpu\n",
    "- that is literally what i asked you\n",
    "- copilot: i'm not sure what you mean by that\n",
    "- -.-\n",
    "- what is the most common data type?\n",
    "- copilot: the most common data type is the float\n",
    "- so like a float32?\n",
    "- copilot: yes\n",
    "- what is the difference between a float and a float32?\n",
    "- copilot: a float is a floating point number, and a float32 is a floating point number with 32 bits\n",
    "- to create a tensor with a specific datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.float32, device(type='cpu'), False)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=None, #the default arg is none, which is automatically a float32\n",
    "                               device=None, #the default arg is none, which is automatically a cpu\n",
    "                               requires_grad=False) #the default arg is false, which means that the tensor will not be used in a gradient computation, if true then operations on the tensor will be recorded\n",
    "\n",
    "float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device, float_32_tensor.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=torch.float16) #torch.half would also work\n",
    "\n",
    "float_16_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8052, 0.5854, 0.5738, 0.2176],\n",
      "        [0.4255, 0.3138, 0.1862, 0.3101],\n",
      "        [0.3033, 0.5568, 0.1510, 0.9652]])\n",
      "shape of some_tensor: torch.Size([3, 4])\n",
      "dtype of some_tensor: torch.float32\n",
      "device of some_tensor: cpu\n"
     ]
    }
   ],
   "source": [
    "some_tensor = torch.rand(3, 4)\n",
    "\n",
    "print(some_tensor)\n",
    "print(f'shape of some_tensor: {some_tensor.shape}')\n",
    "print(f'dtype of some_tensor: {some_tensor.dtype}')\n",
    "print(f'device of some_tensor: {some_tensor.device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), tensor([11, 12, 13]))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#math works on tensors\n",
    "#to create a tensor and add to it\n",
    "tensor = torch.tensor([1,2,3])\n",
    "tensor, tensor + 10\n",
    "#this will add 10 to each element in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to multiply\n",
    "tensor * 10\n",
    "#this will multiply each element by 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensors dont change until they have been reassigned\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9, -8, -7])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor - 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor + 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multiply(tensor, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) * tensor([1, 2, 3])\n",
      "Equals: tensor([1, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "# Element-wise multiplication (each element multiplies its equivalent, index 0->0, 1->1, 2->2)\n",
    "print(tensor, \"*\", tensor)\n",
    "print(\"Equals:\", tensor * tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), 1)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing matrix multplication\n",
    "tensor = torch.tensor([1,2,3]) #one dimensional tensor, or vector wirh a shape of [3]\n",
    "tensor.shape, tensor.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the difference between elemeent wise multiplication\n",
    "and matrix multiplication is the addition of value\n",
    "for tensor i can think about element wise multiplication as\n",
    "[1*1, 2*2, 3*3] = [1,4,9] tensor * tensor\n",
    "and then i can think of a matrix multiplication as [1*1, 2*2, 3*3] = [14]\n",
    "tensor.matmula(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 4, 9]), tensor(14))"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#elementwise, matmulwise\n",
    "tensor * tensor, torch.matmul(tensor, tensor)\n",
    "#this will be useful to quickly do math even graphics like rotating an ascii cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14), tensor(19))"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#its either in python itself or pytorch that the @ \n",
    "#is the symbol for matrix multiplicaion\n",
    "tensor @ tensor, tensor @ torch.tensor([1,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the past algo has told me not to waste time with writing my own matrix multiplication functions\n",
    "this is not to ignore that but to at least know whats under the hood so im not blindly using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#by hand implementation\n",
    "value = 0\n",
    "for i in range(len(tensor)):\n",
    "    value += tensor[i] * tensor[i]\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is obviously much faster\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m tensor_A \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m],\n\u001b[1;32m      3\u001b[0m                          [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m],\n\u001b[1;32m      4\u001b[0m                          [\u001b[39m5\u001b[39m, \u001b[39m6\u001b[39m]], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m      6\u001b[0m tensor_B \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m7\u001b[39m, \u001b[39m10\u001b[39m],\n\u001b[1;32m      7\u001b[0m                          [\u001b[39m8\u001b[39m, \u001b[39m11\u001b[39m], \n\u001b[1;32m      8\u001b[0m                          [\u001b[39m9\u001b[39m, \u001b[39m12\u001b[39m]], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m---> 10\u001b[0m torch\u001b[39m.\u001b[39;49mmatmul(tensor_A, tensor_B) \u001b[39m# (this will error)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "# Shapes need to be in the right way  \n",
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)\n",
    "\n",
    "torch.matmul(tensor_A, tensor_B) # (this will error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this didnt seem obvious to me at first and now i understand\n",
    "what algo and chris meant by touch the tensor and see the shape\n",
    "its becoming clear i coulndt progress without proper pytorch knowlege\n",
    "\n",
    "at first to the eye intuitively these look like something should be able\n",
    "to be done with them and there prolly is but its not what i am trying to\n",
    "do with them right now, the shape is identical, and thats the issue\n",
    "in the case of matrix multiplication the shape has to have the same inner\n",
    "multiplacitively speaking, so [3,2] @ [3,2] wont work but\n",
    "[anything, 2] @ [2, anything] would and if there are errors like this\n",
    "in the future i can just touch the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([3, 2]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_A.shape, tensor_B.shape\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⣿⣿⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⣀⣴⣿⣿⣿⡿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣾⣿⣿⣶⣄⡀⠀⠀⠀⠀⠀⠀\n",
    "                                                    #⠀⣠⣤⣤⣴⣶⣿⣿⣿⣿⠟⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠻⢿⣿⣿⣿⣦⣄⠀⠀⠀⠀\n",
    "                                                    #⢸⣿⣿⣿⣿⡿⠿⠛⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣿⣿⣿⣧⡀⠀⠀\n",
    "                                                    #⠀⠈⠉⠉⠀⠀⠀⠀⠀⢀⣤⣶⣶⣶⣦⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣤⣤⣀⠀⠀⠀⠀⠈⠻⣿⣿⣷⡄⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣿⣿⣿⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠀⣠⣿⣿⣿⣿⣿⣿⣦⠀⠀⠀⠀⠙⣿⣿⣿⡀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⣾⣿⣿⣿⣿⣿⣿⣿⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣿⣿⣿⣿⣿⣷⠀⠀⠀⠀⠹⣿⣿⠇\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠘⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⣿⣿⣿⣿⡟⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀⠀⠀⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠛⠻⠟⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⣿⣿⠏⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠿⣿⣿⡿⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣷⡄⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⠃⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⢀⣤⣄⡀⠀⠀⠀⠀⠀⠀⢀⣠⣴⣶⣶⣶⣦⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⣿⣿⡏⠀⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⢸⣿⣿⣷⣤⡀⠀⠀⣀⣴⣿⣿⣿⡿⠿⢿⣿⣿⣿⣦⡀⠀⠀⠀⠀⠀⢀⣾⣿⣿⡟⠀⠀⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⠟⠁⠀⠀⠀⠉⠻⣿⣿⣿⣦⣀⣀⣀⣴⣿⣿⣿⠟⠀⠀⠀⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⠈⠙⠛⠻⠛⠛⠉⠀⠀⠀⠀⠀⠀⠀⠀⠈⠻⣿⣿⣿⣿⣿⣿⣿⠟⠁⠀⠀⠀⠀⠀⠀⠀\n",
    "                                                    #⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠛⠛⠛⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 7., 10.],\n",
      "        [ 8., 11.],\n",
      "        [ 9., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# View tensor_A and tensor_B\n",
    "print(tensor_A)\n",
    "print(tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# View tensor_A and tensor_B.T\n",
    "print(tensor_A)\n",
    "print(tensor_B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n",
      "\n",
      "New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n",
      "\n",
      "Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) <- inner dimensions match\n",
      "\n",
      "Output:\n",
      "\n",
      "tensor([[ 27.,  30.,  33.],\n",
      "        [ 61.,  68.,  75.],\n",
      "        [ 95., 106., 117.]])\n",
      "\n",
      "Output shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# The operation works when tensor_B is transposed\n",
    "print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\n",
    "print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\n",
    "print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\n",
    "print(\"Output:\\n\")\n",
    "output = torch.matmul(tensor_A, tensor_B.T)\n",
    "print(output) \n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27.,  30.,  33.],\n",
       "        [ 61.,  68.,  75.],\n",
       "        [ 95., 106., 117.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.mm is a shortcut for matmul\n",
    "torch.mm(tensor_A, tensor_B.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear algebra is supposed to be important in machine learning\n",
    "torch is a library for linear algebra\n",
    "torch is a library for tensor computation\n",
    "tensor is a generalization of vectors and matrices\n",
    "a liner linear function can be accurately represented (i think) \n",
    "as **y = x * A^T + b**\n",
    "where x is the input to the layer,\n",
    "A is the weight matrix, created by the layer, it starts out as random\n",
    "numbers that get adjusted as neural networks lean to better represent\n",
    "patterns in the data\n",
    "T is the transpose of A, because the matrix of weights gets transposed\n",
    "b is the bias vector, created by the layer, it starts out as random\n",
    "y is the output of the layer\n",
    "if this is inaccurate dont sue me im wingin this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 3])\n",
      "\n",
      "output:\n",
      "tensor([[0.9332, 0.8805, 3.0149, 1.5545, 1.8186, 2.0634],\n",
      "        [1.7186, 1.4009, 3.5818, 1.7408, 2.6017, 2.5123]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#trying out a linear layer\n",
    "#since it starts as a random weight matrix, i can make it reproduceable wtih a seed\n",
    "torch.manual_seed(42)\n",
    "#this uses matrix multiplication\n",
    "linear = torch.nn.Linear(in_features=2, #in_features is the number of inputs\n",
    "                         out_features=6)  #out_features is the number of outputs, it describes outter values\n",
    "x = tensor_A\n",
    "output = linear(x)\n",
    "print(f\"input shape: {x.shape}\\n\")\n",
    "print(f\"output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if i were to change the in_features to 3, it would throw an error\n",
    "because the input is a 2d tensor, and the in_features is 3\n",
    "it can be fixed with a transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a tensor\n",
    "x = torch.arange(0, 100, 10) #dont forget that arange is arange(start, end, step)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum:0, maximum:90\n",
      "mean:45.0\n",
      "sum:450\n"
     ]
    }
   ],
   "source": [
    "print(f\"minimum:{x.min()}, maximum:{x.max()}\")\n",
    "# print(F'mean:{x.mean()}) #this will error\n",
    "print(f'mean:{x.type(torch.float32).mean()}') #this operation wont work if i just do {x.mean()}\n",
    "print(f'sum:{x.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(90), tensor(45.), tensor(450))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the follwing will work the same as above\n",
    "torch.min(x), torch.max(x), torch.mean(x.type(torch.float32)), torch.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
      "index where max value is 8\n",
      "index where min value is 0\n"
     ]
    }
   ],
   "source": [
    "#to learn indexing i have to create a new tensor\n",
    "tensor = torch.arange(10,100,10)\n",
    "print(tensor)\n",
    "print(f'index where max value is {tensor.argmax()}')\n",
    "print(f'index where min value is {tensor.argmin()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new tensor(prolly gonna do this a ton)\n",
    "tensor = torch.arange(10.,100.,10.)\n",
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a float 16 tensor\n",
    "tensorfloat16 = tensor.type(torch.float16) #type will create a copy of the tensor fed to it through it and convert it to the type specified\n",
    "tensorfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorint8 = tensor16.type(torch.int8)\n",
    "tensorint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a tensor\n",
    "x = torch.arange(1.,8.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#im going to try adding dimension with reshape()\n",
    "x_reshape = x.reshape(1,7)\n",
    "x_reshape, x_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another way to change the shape of a tensor is to use .view() method\n",
    "z = x.view(1,7)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick ref will return to later\n",
    "Assume x is a 2-dimensional tensor (or a matrix). Here are some ways you can index into x:\n",
    "Select a specific element: x[i, j] will select the element at the i-th row and j-th column.\n",
    "Select a specific row or column: x[i, :] will select the i-th row, and x[:, j] will select the j-th column.\n",
    "Select multiple rows or columns: x[1:3, :] will select rows 1 and 2 (note that the end index is exclusive), \n",
    "and x[:, 1:3] will select columns 1 and 2.\n",
    "Select with a step: x[::2, :] will select every other row, starting from row 0.\n",
    "Select with negative indices: x[-1, :] will select the last row. Negative indices count from the end.\n",
    "Use boolean indexing: If you have a tensor of the same shape as x, but with boolean values (i.e., True and False),\n",
    "you can use this as an index. x[boolean_tensor] will select all elements of x where the corresponding value in the boolean tensor is True.\n",
    "Use integer indexing: You can create a tensor of integers to select multiple rows or columns in a specific order or multiple times.\n",
    "x[[1, 2, 1], :] will select the second, third, and second rows of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:,0] = 5\n",
    "z,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack tensors on top of each other\n",
    "x_stacked = torch.stack([x, x, x, x], dim=0) #stack is used to stack tensors on top of each other, which adds a new dimension\n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous tensor:tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "previous shape:torch.Size([1, 7])\n",
      "new tensor:tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "new shape:torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "#to remove all single dimensions from a tensor use torch.squeeze()\n",
    "print(f'previous tensor:{x_reshape}')\n",
    "print(f'previous shape:{x_reshape.shape}')\n",
    "x_squeezed = torch.squeeze(x_reshape) #torch.squeeze() removes all single dimensions from a tensor in other words it removes anythihng that has a size of 1\n",
    "print(f'new tensor:{x_squeezed}')\n",
    "print(f'new shape:{x_squeezed.shape}')\n",
    "#youre squeezing the tensor to only have dimensions over 1, or squeezing out all the 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "tensor([[5., 2., 3., 4., 5., 6., 7.]])\n"
     ]
    }
   ],
   "source": [
    "print(x_squeezed)\n",
    "xunsqueezed = x_squeezed.unsqueeze(dim=0) #this adds a dimension to a specific index\n",
    "print(xunsqueezed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i can also rearrange the order of axes values with permute(input, dims)\n",
    "where input gets turned into a view with new dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([244, 244, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create a tensor with a specific shape\n",
    "x_og = torch.rand(size=(244,244,3))\n",
    "\n",
    "#permute the og tensor to rearrange the axis order\n",
    "x_perm = x_og.permute(2,0,1)\n",
    "\n",
    "print(x_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " .-._                                                   _,-,\n",
    "  `._`-._                                           _,-'_,'\n",
    "     `._ `-._                                   _,-' _,'\n",
    "        `._  `-._        __.-----.__        _,-'  _,'\n",
    "           `._   `#===\"\"\"           \"\"\"===#'   _,'\n",
    "              `._/)  ._               _.  (\\_,'\n",
    "               )*'     **.__     __.**     '*( \n",
    "               #  .==..__  \"\"   \"\"  __..==,  # \n",
    "BOO            #   `\"._(_).       .(_)_.\"'   #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# indexing(selecting data from tensors)\n",
    "to select specific data from tensors(like only the first column or second row)\n",
    "to do so i can index like in vanilla python or numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [4, 5, 6],\n",
       "          [7, 8, 9]]]),\n",
       " torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a tensor\n",
    "import torch\n",
    "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indexing the values of a tensor goes: outer dimension -> inner dimension (check brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first square bracket\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "second square bracket\n",
      "tensor([1, 2, 3])\n",
      "third square bracket\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#indexing bracket by bracket\n",
    "print(f'first square bracket\\n{x[0]}')\n",
    "print(f'second square bracket\\n{x[0][0]}')\n",
    "print(f'third square bracket\\n{x[0][0][0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on god for real for real, i feel like this is probnobably going to be the most im[port]ant thing i have written so far. it seems like all of the\n",
    "cool stuff ive seen algo do is like really sick indexing, the idea of saying \"really sick indexing\" is making me wanna **play sudoku**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i can also use : to specify \"all values in this dimension\" and then use a comma to add another dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "#get all values of the 0th dimension and the 0 index of the 1st dimension\n",
    "print(x[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 5, 8]])\n"
     ]
    }
   ],
   "source": [
    "#get all the values of the 0th and 1st dimensions but only the index 1 of the 2nd dimension\n",
    "print(x[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n",
    "x[:, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get index 0 of 0th and 1st dimension and all values of 2nd dimension \n",
    "x[0, 0, :] # same as x[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since numpy is so popular pytorch is made to interface with it well\n",
    "two main methods to make early note of are:\n",
    "- torch.from_numpy(ndarray) - numpy array to pytorch tensor\n",
    "- torch.Tensor.numpy() - pytorch tensor to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numpy to tensor\n",
    "import numpy as np\n",
    "\n",
    "array = np.arange(1.0, 8.0)\n",
    "tensor = torch.from_numpy(array)\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by default numpy arrays are created with the datatype float64 and if you convert it to a pytorch tensor it will keep the same datatype.\n",
    "however because so many tensors are float32 i can convert the array with type after its been made into a tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 3., 4., 5., 6., 7., 8.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change array keep tensor\n",
    "array = array + 1\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor to numpy\n",
    "tensor = torch.ones(7)\n",
    "numpy_tensor = tensor.numpy()\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change tensor keep the array\n",
    "tensor = tensor + 1\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reproducability is important in machine learning and data science so i need to learn how to use randomness in a reproducable way\n",
    "start with random #'s => tensor operations => improve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor a: tensor([[0.8694, 0.5677, 0.7411, 0.4294],\n",
      "        [0.8854, 0.5739, 0.2666, 0.6274],\n",
      "        [0.2696, 0.4414, 0.2969, 0.8317]])\n",
      "\n",
      "tensor b: tensor([[0.1053, 0.2695, 0.3588, 0.1994],\n",
      "        [0.5472, 0.0062, 0.9516, 0.0753],\n",
      "        [0.8860, 0.5832, 0.3376, 0.8090]])\n",
      "\n",
      "does tensor a = tensor b anywhere?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create two random tensors\n",
    "randa = torch.rand(3,4)\n",
    "randb = torch.rand(3,4)\n",
    "cat =                  \"\"\" ╱|、\n",
    "                          (˚ˎ 。7  \n",
    "                          |、 ˜ 〵          \n",
    "                          じしˍ,)ノ🐈🐈\"\"\"\n",
    "\n",
    "print(f'tensor a: {randa}\\n')\n",
    "print(f'tensor b: {randb}\\n')\n",
    "print(f'does tensor a = tensor b anywhere?')\n",
    "randa == randb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these are different even though they both look identical because they are random from a different seed\n",
    "i can use torch.manual_seed(seed) to set the seed to the same place every time so the sensation of random is aligned in that instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomtensor: tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "randomtensor2: tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "does randomtensor == randomtensor2? tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#set the random seed\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "randomtensor = torch.rand(3, 4)\n",
    "                                    # its important to set the seed before creating the tensor each time\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "randomtensor2 = torch.rand(3, 4)\n",
    "\n",
    "print(f'randomtensor: {randomtensor}')\n",
    "print(f'randomtensor2: {randomtensor2}')\n",
    "print(f'does randomtensor == randomtensor2? {randomtensor == randomtensor2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣤⣤⣤⣤⣤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⣴⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ you can do all of this stuff on cpu or gpu if you have one⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⣿⠿⠋⠉⠁⠀⠀⠀⠹⢿⡿⣿⡻⠟⠿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⡿⡃⠀⠀⠀⠀⠀⠀⠀⠰⡄⢹⣿⡇⠀⢻⡿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣻⠁⠀⠀⠀⠀⠀⠀⠀⠀⣛⣼⡿⠃⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣶⣿⣿⣷⣶⣦⣤⣀⡀⠀⠀                  running tensors on gpus\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣿⡇⠻⣿⣤⣀⠀⠀⢀⣠⣤⡶⠟⠛⠁⠀⠀⠀⠈⠙⠓⠲⠶⠦⣤⣤⣄⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⠟⠋⢙⣿⣿⣿⣿⣿⣿⣿⣿⣶⣄               lets the computer do all the\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠹⣿⣦⠈⠉⠛⠛⠛⠋⠉⠀⠀⢀⣀⣀⣀⣠⣤⣤⣤⣤⣤⣤⣀⣀⣀⣀⠈⠉⠙⠓⠲⠶⢤⣀⡀⠀⠀⠀⡏⠀⢠⣾⠋⠁⠛⣉⡈⠙⠻⢿⣿⣿              calculations way more better\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠙⣿⠇⠀⢀⣀⣴⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⣦⣄⣀⠀⠉⠉⠛⠛⠃⠀⠀⣿⡄⠀⠀⠀⠀⠀⠀⠀⠙⢿               so that everything is done\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⡾⢃⣤⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣄⡀⠀⠀⠀⠉⢷⣄⠀⠀⠀⠀⠀⠀⠀⢠               faster and more efficient\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣯⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⡀⠀⠀⠙⢷⣄⠀⠀⠀⠀⠀⠀\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⣿⣿⣿⣿⣿⣿⠟⠋⠉⠉⠉⠉⠛⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⠀⠀⠙⠳⣤⣀⠀⠀⣐\\n⠀⠀⠀⠀⠀⠀⠀⠀⣰⢟⣿⣿⣿⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠈⠉⠛⠛⠉\\n⠀⠀⠀⠀⠀⠀⠀⣼⢏⣾⣿⣿⣿⣿⣿⡟⠛⠛⣶⣄⣀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡀⠀⠀⢈⣼⡿\\n⠀⠀⠀⠀⠀⠀⣸⠏⣾⣿⣿⣿⣿⣿⣿⠀⠀⢰⡏⠉⠙⠻⠷⣶⣤⣄⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠛⠋⠉⠛⠻⣿⣿⣿⣿⣿⣄⠀⢿⠁⠀\\n⠀⠀⠀⠀⠀⣴⡏⣸⣿⣿⣿⣿⣿⣿⡏⠀⠀⢸⡇⠀⠀⠀⣠⡿⠃⠈⣽⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠃⠀⠀⠀⠀⠀⠀⠈⢻⣿⣿⣿⣿⣆⠸⣇⠀\\n⠀⠀⠀⠀⣰⡟⢰⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠘⢷⣤⣤⠾⠋⠀⠀⣰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⡆⢹⡀\\n⠀⠀⠀⣰⣿⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣆⠀⠀⠀⠀⠀⠀⠀⣀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠷⠤⣤⣤⣤⣤⣤⣤⣤⣀⣀⣸⣿⣿⣿⣿⣷⡈⢧\\n⠀⠀⠀⣿⡇⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣤⣤⣤⣴⣶⣿⣿⣿⣿⣿⣿⣿⠿⠿⠿⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⢹⡆⠀⠀⠀⢈⣽⠃⠀⢹⣿⣿⣿⣿⣿⡇⠈\\n⠀⠀⠀⣿⡇⠀⠈⢿⣿⣿⣿⣿⠿⠿⢿⣽⡟⣿⣿⣿⣿⣿⣿⡿⠟⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠉⠛⠻⢿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠈⢷⡄⢀⣠⡿⠁⠀⢀⣿⣿⣿⣿⣿⣿⡇⠀                 \\n⠀⠀⠀⣿⢑⣂⣠⣤⣿⣿⡷⢶⣶⣶⠀⠙⣿⢻⣿⡿⠟⠉⠁⠀⠀⠀⠀⢀⣤⣴⣶⣶⣶⣦⣤⣀⠀⠀⠀⠈⠛⢿⣿⣿⣿⣿⣧⠀⠀⠀⠙⠛⠉⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⡇⠀\\n⠀⠀⠀⣿⣾⣿⣿⣏⠀⠀⠀⠀⠀⠀⢀⣼⠋⢸⡇⠀⠀⠀⠀⠀⠀⠀⣤⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⡄⠀⠀⠀⠙⢿⣿⣿⣿⣷⣤⡀⠀⠀⠀⠀⣠⣾⣿⣿⣿⣿⣿⣿⣿⠇⠀\\n⠀⢀⣠⣿⣿⣿⡿⠿⠿⠀⠀⠀⢀⣴⡿⠁⠀⣾⡇⠀⠀⠀⠀⠀⢀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠹⢿⣿⣿⣿⣿⣿⣶⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠀⠀\\n⣾⣿⣿⣿⣟⣿⣿⣷⣄⠀⢀⣴⣿⠟⠀⢀⣼⢟⣿⣀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀\\n⣿⣿⢛⣻⣝⡟⢿⣟⢿⣷⣨⡿⠃⠀⢠⣿⡋⢸⣿⠛⠻⢶⣄⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠒⠀⠀⠀⠀⠀⠀⠀⠈⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠏⠀⠀⠀     \\n⡿⣽⣿⣿⣿⣿⣆⢻⡏⣿⡏⠀⠀⢀⣾⢋⣷⣾⣿⣦⡀⠀⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠿⣿⣿⣿⣿⣿⣿⣿⠟⠃⠀⠀⠀⣸\\n⡵⣿⣿⣿⣿⣿⣿⠄⣿⣿⡇⠀⣶⣿⣿⣸⣿⣿⣿⣿⣿⡆⠀⠀⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡇\\n⣧⡻⣿⣿⣿⣿⠟⣼⡏⣿⠇⠜⣰⣏⣿⣿⣿⣿⣿⣿⡏⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⡿⠀\\n⠈⠻⣦⣤⣤⣤⠞⢙⣹⡿⠀⣰⡿⠛⠿⢿⣿⣿⣿⡿⠀⠀⠀⠀⠀⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡿⠁⠀\\n⠀⠀⣠⣬⣭⡀⠀⢿⡏⠀⣴⣯⠁⠀⠒⠒⢾⣿⣿⡇⠀⣀⣀⡀⢠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣤⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣤⣾⠟⠃⠀⠀\\n⣤⣿⣿⣿⣷⣔⣖⣼⠀⣰⣿⢹⢀⣄⠀⠀⢸⣽⣿⡇⠀⠀⠀⢨⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣶⣶⣶⣶⣶⣿⣿⣿⠟⠋⠁⠀⠀⠀⠀\\n⢿⠸⣿⣿⣿⡿⣽⡿⣰⣿⢃⣿⠮⣽⣶⣴⡿⠿⠟⢧⡀⠀⠀⣿⠋⠉⠉⠙⠛⠛⠿⠿⠿⠿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⠋⠀⠀⠀⠀⠀⠀⠀⠀\\n⣧⡷⣌⣉⣉⣥⣿⣵⡏⣤⣿⠇⠀⠀⠀⠀⠀⠀⠀⠈⢿⡷⠾⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠳⠦⣤⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n⣿⣿⣶⣤⣤⣴⣿⣿⡿⠛⠁⠀⠀⠀⠀⠀⠀⠀⣀⢀⣋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠛⠶⢤⣤⣀⣀'"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣤⣤⣤⣤⣤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⣴⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ you can do all of this stuff on cpu or gpu if you have one⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⣿⠿⠋⠉⠁⠀⠀⠀⠹⢿⡿⣿⡻⠟⠿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⡿⡃⠀⠀⠀⠀⠀⠀⠀⠰⡄⢹⣿⡇⠀⢻⡿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣻⠁⠀⠀⠀⠀⠀⠀⠀⠀⣛⣼⡿⠃⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣶⣿⣿⣷⣶⣦⣤⣀⡀⠀⠀                  running tensors on gpus\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣿⡇⠻⣿⣤⣀⠀⠀⢀⣠⣤⡶⠟⠛⠁⠀⠀⠀⠈⠙⠓⠲⠶⠦⣤⣤⣄⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⠟⠋⢙⣿⣿⣿⣿⣿⣿⣿⣿⣶⣄               lets the computer do all the\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠹⣿⣦⠈⠉⠛⠛⠛⠋⠉⠀⠀⢀⣀⣀⣀⣠⣤⣤⣤⣤⣤⣤⣀⣀⣀⣀⠈⠉⠙⠓⠲⠶⢤⣀⡀⠀⠀⠀⡏⠀⢠⣾⠋⠁⠛⣉⡈⠙⠻⢿⣿⣿              calculations way more better\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠙⣿⠇⠀⢀⣀⣴⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⣦⣄⣀⠀⠉⠉⠛⠛⠃⠀⠀⣿⡄⠀⠀⠀⠀⠀⠀⠀⠙⢿               so that everything is done\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⡾⢃⣤⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣄⡀⠀⠀⠀⠉⢷⣄⠀⠀⠀⠀⠀⠀⠀⢠               faster and more efficient\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣯⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⡀⠀⠀⠙⢷⣄⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⣿⣿⣿⣿⣿⣿⠟⠋⠉⠉⠉⠉⠛⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⠀⠀⠙⠳⣤⣀⠀⠀⣐\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⣰⢟⣿⣿⣿⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠈⠉⠛⠛⠉\n",
    "⠀⠀⠀⠀⠀⠀⠀⣼⢏⣾⣿⣿⣿⣿⣿⡟⠛⠛⣶⣄⣀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡀⠀⠀⢈⣼⡿\n",
    "⠀⠀⠀⠀⠀⠀⣸⠏⣾⣿⣿⣿⣿⣿⣿⠀⠀⢰⡏⠉⠙⠻⠷⣶⣤⣄⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠛⠋⠉⠛⠻⣿⣿⣿⣿⣿⣄⠀⢿⠁⠀\n",
    "⠀⠀⠀⠀⠀⣴⡏⣸⣿⣿⣿⣿⣿⣿⡏⠀⠀⢸⡇⠀⠀⠀⣠⡿⠃⠈⣽⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠃⠀⠀⠀⠀⠀⠀⠈⢻⣿⣿⣿⣿⣆⠸⣇⠀\n",
    "⠀⠀⠀⠀⣰⡟⢰⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠘⢷⣤⣤⠾⠋⠀⠀⣰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⡆⢹⡀\n",
    "⠀⠀⠀⣰⣿⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣆⠀⠀⠀⠀⠀⠀⠀⣀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠷⠤⣤⣤⣤⣤⣤⣤⣤⣀⣀⣸⣿⣿⣿⣿⣷⡈⢧\n",
    "⠀⠀⠀⣿⡇⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣤⣤⣤⣴⣶⣿⣿⣿⣿⣿⣿⣿⠿⠿⠿⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⢹⡆⠀⠀⠀⢈⣽⠃⠀⢹⣿⣿⣿⣿⣿⡇⠈\n",
    "⠀⠀⠀⣿⡇⠀⠈⢿⣿⣿⣿⣿⠿⠿⢿⣽⡟⣿⣿⣿⣿⣿⣿⡿⠟⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠉⠛⠻⢿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠈⢷⡄⢀⣠⡿⠁⠀⢀⣿⣿⣿⣿⣿⣿⡇⠀                 \n",
    "⠀⠀⠀⣿⢑⣂⣠⣤⣿⣿⡷⢶⣶⣶⠀⠙⣿⢻⣿⡿⠟⠉⠁⠀⠀⠀⠀⢀⣤⣴⣶⣶⣶⣦⣤⣀⠀⠀⠀⠈⠛⢿⣿⣿⣿⣿⣧⠀⠀⠀⠙⠛⠉⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⣿⣾⣿⣿⣏⠀⠀⠀⠀⠀⠀⢀⣼⠋⢸⡇⠀⠀⠀⠀⠀⠀⠀⣤⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⡄⠀⠀⠀⠙⢿⣿⣿⣿⣷⣤⡀⠀⠀⠀⠀⣠⣾⣿⣿⣿⣿⣿⣿⣿⠇⠀\n",
    "⠀⢀⣠⣿⣿⣿⡿⠿⠿⠀⠀⠀⢀⣴⡿⠁⠀⣾⡇⠀⠀⠀⠀⠀⢀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠹⢿⣿⣿⣿⣿⣿⣶⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠀⠀\n",
    "⣾⣿⣿⣿⣟⣿⣿⣷⣄⠀⢀⣴⣿⠟⠀⢀⣼⢟⣿⣀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀\n",
    "⣿⣿⢛⣻⣝⡟⢿⣟⢿⣷⣨⡿⠃⠀⢠⣿⡋⢸⣿⠛⠻⢶⣄⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠒⠀⠀⠀⠀⠀⠀⠀⠈⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠏⠀⠀⠀     \n",
    "⡿⣽⣿⣿⣿⣿⣆⢻⡏⣿⡏⠀⠀⢀⣾⢋⣷⣾⣿⣦⡀⠀⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠿⣿⣿⣿⣿⣿⣿⣿⠟⠃⠀⠀⠀⣸\n",
    "⡵⣿⣿⣿⣿⣿⣿⠄⣿⣿⡇⠀⣶⣿⣿⣸⣿⣿⣿⣿⣿⡆⠀⠀⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡇\n",
    "⣧⡻⣿⣿⣿⣿⠟⣼⡏⣿⠇⠜⣰⣏⣿⣿⣿⣿⣿⣿⡏⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⡿⠀\n",
    "⠈⠻⣦⣤⣤⣤⠞⢙⣹⡿⠀⣰⡿⠛⠿⢿⣿⣿⣿⡿⠀⠀⠀⠀⠀⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡿⠁⠀\n",
    "⠀⠀⣠⣬⣭⡀⠀⢿⡏⠀⣴⣯⠁⠀⠒⠒⢾⣿⣿⡇⠀⣀⣀⡀⢠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣤⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣤⣾⠟⠃⠀⠀\n",
    "⣤⣿⣿⣿⣷⣔⣖⣼⠀⣰⣿⢹⢀⣄⠀⠀⢸⣽⣿⡇⠀⠀⠀⢨⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣶⣶⣶⣶⣶⣿⣿⣿⠟⠋⠁⠀⠀⠀⠀\n",
    "⢿⠸⣿⣿⣿⡿⣽⡿⣰⣿⢃⣿⠮⣽⣶⣴⡿⠿⠟⢧⡀⠀⠀⣿⠋⠉⠉⠙⠛⠛⠿⠿⠿⠿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⠋⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⣧⡷⣌⣉⣉⣥⣿⣵⡏⣤⣿⠇⠀⠀⠀⠀⠀⠀⠀⠈⢿⡷⠾⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠳⠦⣤⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣶⣤⣤⣴⣿⣿⡿⠛⠁⠀⠀⠀⠀⠀⠀⠀⣀⢀⣋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠛⠶⢤⣤⣀⣀\"\"\" #give me a gpu or else (i will cry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Aug  8 15:10:05 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 536.25                 Driver Version: 536.25       CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080      WDDM  | 00000000:06:00.0  On |                  N/A |\n",
      "| 50%   75C    P0             242W / 370W |   2364MiB / 10240MiB |     71%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      5944    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A      6084    C+G   ...A Broadcast\\NVIDIA Broadcast UI.exe    N/A      |\n",
      "|    0   N/A  N/A     11116    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     11180    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12808    C+G   ...5n1h2txyewy\\ShellExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13540    C+G   ...am Files\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "|    0   N/A  N/A     14184    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14436    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     14728    C+G   ...cal\\Microsoft\\OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     17480    C+G   ...GeForce Experience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     18680    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     19800    C+G   ...al\\Discord\\app-1.0.9016\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     20072    C+G   ...ejd91yc\\AdobeNotificationClient.exe    N/A      |\n",
      "|    0   N/A  N/A     20256    C+G   ...\\cef\\cef.win7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A     20704    C+G   ...B\\system_tray\\lghub_system_tray.exe    N/A      |\n",
      "|    0   N/A  N/A     20804    C+G   C:\\Program Files\\LGHUB\\lghub.exe          N/A      |\n",
      "|    0   N/A  N/A     21880    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     23136    C+G   ...\\Programs\\signal-desktop\\Signal.exe    N/A      |\n",
      "|    0   N/A  N/A     25368    C+G   ...les\\Proton\\VPN\\v3.0.7\\ProtonVPN.exe    N/A      |\n",
      "|    0   N/A  N/A     26572    C+G   ...s\\Sony\\Imaging Edge Desktop\\ied.exe    N/A      |\n",
      "|    0   N/A  N/A     26664    C+G   ...50.0_x64__v10z8vjag6ke6\\HP.myHP.exe    N/A      |\n",
      "|    0   N/A  N/A     28424    C+G   ...n\\115.0.1901.188\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     31472    C+G   ...61.0_x64__8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "|    0   N/A  N/A     33664    C+G   ...paper_engine\\bin\\webwallpaper32.exe    N/A      |\n",
      "|    0   N/A  N/A     33712    C+G   ...al\\Discord\\app-1.0.9016\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     35148    C+G   ...t.LockApp_cw5n1h2txyewy\\LockApp.exe    N/A      |\n",
      "|    0   N/A  N/A     37368    C+G   ...n\\115.0.1901.188\\msedgewebview2.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "#to check gpu\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im running all of this mainly on my desktop pc which is a windows machine so if you run this\n",
    "(largely speaking to myself in the future when im going over all of this on my macbook pro)\n",
    "and you dont have an nvidja gpu youll get some message like:\n",
    "- NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
    "if you have an nvidja gpu youll see something like this but not as cool:\n",
    "```Tue Aug  8 15:10:05 2023       \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 536.25                 Driver Version: 536.25       CUDA Version: 12.2     |\n",
    "|-----------------------------------------+----------------------+----------------------+\n",
    "| GPU  Name                     TCC/WDDM  | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                      |               MIG M. |\n",
    "|=========================================+======================+======================|\n",
    "|   0  NVIDIA GeForce RTX 3080      WDDM  | 00000000:06:00.0  On |                  N/A |\n",
    "| 50%   75C    P0             242W / 370W |   2364MiB / 10240MiB |     71%      Default |\n",
    "|                                         |                      |                  N/A |\n",
    "+-----------------------------------------+----------------------+----------------------+\n",
    "                                                                                         \n",
    "+---------------------------------------------------------------------------------------+\n",
    "| Processes:                                                                            |\n",
    "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
    "|        ID   ID                                                             Usage      |```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for gpu\n",
    "torch.cuda.is_available() #cuda means the gpu idfk why\n",
    "#also, my cuda is not correctly installed, so it's false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if the cuda output is true i need to remember to set the device type every time i make something using these ideas so that the math is done on the right processors\n",
    "and i should make sure to make it dynamic so it doesnt become machine dependant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count number of devices                    #  ╱|、 \n",
    "torch.cuda.device_count()                   #(˚ˎ 。7  \n",
    "                                            #|、 ˜ 〵             \n",
    "                                            #じしˍ,)ノ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "knowing the # of gpu's pytorch has access to is usefull and so is being able to put the tensor or \n",
    "models onto the gpu ≽^•⩊•^≼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a tensor\n",
    "tensor = torch.tensor([1,2,3])\n",
    "#tensor not on gpu\n",
    "print(tensor, tensor.device)\n",
    "\n",
    "#move tensor to gpu\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if a tensor is on gpu cant transform it to numpy this willl error\n",
    "tensor_on_gpu.numpy() #im running on a cpu for this time so it wont error on me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numpy is not able to use the gpu so you have to copy the tensor back to the cpu with tensor.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3], dtype=int64), tensor([1, 2, 3]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing that out\n",
    "tensor_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_on_cpu, tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensor recapp\n",
    "- a torch.tensor is a multidimensional matrix containing elements of a single data type\n",
    "- torch defines ten tensor types with cpu and gpu variants as follows:\n",
    "    - 32 bit floating point\n",
    "    - 64 bit floating point\n",
    "    - 16 bit floating point\n",
    "    - 32 bit complex\n",
    "    - 64 bit complex\n",
    "    - 128 bit complex\n",
    "    - 8 bit integer (unsigned)\n",
    "    - 8 bit integer (signed)\n",
    "    - 16 bit integer (signed)\n",
    "    - 32 bit integer (signed)\n",
    "    - 64 bit integer (signed)\n",
    "    - boolean\n",
    "    - quantized 8 bit integer (unsigned)\n",
    "    - quantized 8 bit integer (signed)\n",
    "    - quantized 32 bit integer (signed)\n",
    "    -quantized 4 bit integer (unsigned)\n",
    "\n",
    "- a tensor can be constructed from a python list or sequence using the torch.tensor() constructor:\n",
    "    ```\n",
    "    >>> torch.tensor([[1., -1.], [1., -1.]])\n",
    "    tensor([[ 1.0000, -1.0000],\n",
    "            [ 1.0000, -1.0000]])\n",
    "    >>> torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "    tensor([[ 1,  2,  3],\n",
    "            [ 4,  5,  6]])\n",
    "    ```\n",
    "\n",
    "# remember dimensions work like () ([]) ([[]]) etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣀⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀that was a lot\n",
    "- ⠀⠀⠀⠀⠀⠀⢀⣠⠤⠖⠈⠉⠉⠀⠀⠀⠀⠉⠢⡀⠀⠀⠀⠀⠀⠀\n",
    "- ⠀⠀⠀⠀⠀⣴⠏⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⢦⡀⠀⠀⠀⠀\n",
    "- ⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⠞⠋⢙⣦⡈⣷⡄⠀⠀⠀\n",
    "- ⠀⣀⠶⠁⠀⠀⣀⣀⡀⠀⠀⠀⠀⠀⡴⠁⠀⠀⠿⢿⡟⣌⢿⠀⠀⠀\n",
    "- ⣠⡿⠀⢠⣜⠉⠀⠀⠙⢷⢄⠀⠀⠀⢧⠀⠀⠀⠀⠀⠀⠘⡆⢧⡀⠀\n",
    "- ⣯⠃⠀⢾⣿⠗⠀⠀⠀⠀⡽⠀⠀⠀⠈⠳⢄⣀⠀⠀⠀⡰⠃⠘⣵⡄\n",
    "- ⡏⠀⠀⠘⡄⠀⠀⠀⣠⠞⠁⠀⠀⠀⠀⠀⠀⠀⠉⠉⠁⠀⠀⠀⢱⡇\n",
    "- ⡅⠀⠀⠀⠙⠒⠔⠚⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⡇\n",
    "- ⣧⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⠀⠀⠀⠀⠀⠀⠀⡗\n",
    "- ⡿⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡀⠀⠀⠀⠀⢸⡇⠀⠀⠀⠀⠀⠀⣇\n",
    "- ⠹⣷⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠷⣤⣤⣤⣤⠞⠁⠀⠀⠀⠀⠀⠀⣸\n",
    "- ⠀⠸⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⠇\n",
    "- ⠀⠀⢇⠳⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡏⠀\n",
    "- ⠀⠀⠈⠀⠀⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5120, 0.5831, 0.6033, 0.2437, 0.6726, 0.9717, 0.1395],\n",
       "        [0.3271, 0.4676, 0.7492, 0.1786, 0.7175, 0.3222, 0.7436],\n",
       "        [0.5395, 0.9417, 0.1715, 0.0342, 0.0095, 0.0849, 0.8330],\n",
       "        [0.0595, 0.4170, 0.6811, 0.2070, 0.2354, 0.7237, 0.2270],\n",
       "        [0.1209, 0.5735, 0.1444, 0.0646, 0.1477, 0.0227, 0.8963],\n",
       "        [0.3300, 0.1252, 0.3733, 0.5784, 0.9826, 0.3131, 0.1794],\n",
       "        [0.8224, 0.8285, 0.3302, 0.8742, 0.3631, 0.9011, 0.7905]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just practicing\n",
    "#create a random tensor with a shape (7,7)\n",
    "import torch\n",
    "tensor = torch.rand(7,7)\n",
    "tensor #ᓚᘏᗢ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.7176],\n",
      "        [1.8792],\n",
      "        [1.7753],\n",
      "        [1.0299],\n",
      "        [1.2949],\n",
      "        [1.5860],\n",
      "        [2.9291]])\n"
     ]
    }
   ],
   "source": [
    "#preform matrix multiplication on the tensor with another tensor rand shape 1,7 \n",
    "nother_tensor = torch.rand(1,7).T\n",
    "answer = torch.mm(tensor, nother_tensor)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i just realized i have litterally no way of knowing if i have learned anything yet or if any of this is of use to me so please if you are following along tune into things i build after this study session to make sure im not saying random bullshit\n",
    "this is all just what i can study on my own in free time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5985],\n",
      "        [1.1173],\n",
      "        [1.2741],\n",
      "        [1.6838],\n",
      "        [0.8279],\n",
      "        [1.0347],\n",
      "        [1.2498]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "new_tensor = torch.rand(7,7)\n",
    "torch.manual_seed(0)\n",
    "new_nother_tensor = torch.rand(1,7)\n",
    "print(torch.mm(new_tensor, new_nother_tensor.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# refining and developing a workflow seems SUPER IMPORTANT \n",
    "machine learning is like building little autistic ppl out of code to handle data the way big autistic ppl made out of meat do\n",
    "    - you take some data from the past, build an algorithm, and discover patterns and predict the future\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## im going to be building a small torch model that learns the pattern of a straight line and matches it\n",
    "### im going to be going over the following through this section of my study notes:\n",
    "    - getting data ready => Data can be almost anything but to get started we're going to create a simple straight line\n",
    "    -building a model => ill create a model to learn patterns in the data and choose a loss function, optimizer, and build a training loop to get\n",
    "    a more ground up hands on approach to the conceptual understanding of each\n",
    "    -fitting the model to data => train the model to find patterns in the data\n",
    "    - make predictions and evaluate them => the model will find patterns and output predictions so i will have to compare them to the targets\n",
    "    - saving and loading a model => i have litterally no idea how to do this so this will be cool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'data (prep and load)',\n",
       " 2: 'build model',\n",
       " 3: 'train',\n",
       " 4: 'inference',\n",
       " 5: 'saving and loading a model',\n",
       " 6: 'put it together'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "what_im_building = {1: \"data (prep and load)\",\n",
    "                    2: \"build model\",\n",
    "                    3: \"train\",\n",
    "                    4: \"inference\",\n",
    "                    5: \"saving and loading a model\",\n",
    "                    6: \"put it together\"\n",
    "                    }\n",
    "what_im_building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#check pytorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "an issue with what im trying to do right now i thnk is that i want to build a model and a model has data and i dont have any starting data so i have to make it, so this is the starting data for a straight line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]))"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creating known parameters\n",
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "#create data\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = weight * X + bias\n",
    "print(X.shape)\n",
    "\n",
    "X[:10], y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i have now created some data to start the process with and am about to move on to building a moodel that can learn the relationship between:\n",
    "- X (features)\n",
    "- y (labels)\n",
    "\n",
    "before i build the model i need to split up this data and make a training and test set and then when i need it a validation set\n",
    "each split of the dataset serves a specific purpose:\n",
    "- training set => the model will learn from this => ~60-80% of the data => always used\n",
    "- validation set => the model gets tuned on this data => ~10-20% of the data => sometimes used\n",
    "- testing set => the model gets evaluated on this data to test its learning => ~10-20% => always\n",
    "\n",
    "for now all i need is a training and test set so i just need a data set for the model to learn on and be evaluated on\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the dataset\n",
    "train_split = int(0.8 * len(X)) # 80% of the data for training\n",
    "print(train_split)\n",
    "X_train, y_train = X[:train_split], y[:train_split]\n",
    "X_test, y_test = X[train_split:], y[train_split:]\n",
    "\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is 40 samples for training, 10 for testing, a set for each X and y\n",
    "now i need to visualize this because while i get what this **is**, i dont get what this is at all like its clear im just creating a tensor and creating a way to index into it to create sets of data for stuff, but thats not really usefull without gaining more conceptual ground for me and i think visualizing it is a great way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data=X_train,  \n",
    "                    train_labels=y_train,\n",
    "                    test_data=X_test,\n",
    "                    test_labels=y_test,\n",
    "                    predictions=None,):\n",
    "    \"\"\"Plots training data, test data and compares predictions if provided.\"\"\"\n",
    "    plt.figure(figsize=(10,7)) #makes a new plot (graph thingy in python) with a width of 10 and height of 7\n",
    "    # Plot training data in blue, the size of each point is set to 4 and the label training data is assigned\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "    # Plot test data in green, the size of each point is set to 4 and the label test data is assigned\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "    # If predictions are provided, plot them in red (predictions were made on the test data)\n",
    "    if predictions is not None:\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
    "\n",
    "    plt.legend(prop={\"size\": 14})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i am not the best with matplotlib yet that will be another library i study after pytorch but i am gonna try my best to understand this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so this function when called visualizes a plot with training data, test data, and predicted data (if there was any)  giving a visual comparison between the datasets\n",
    "\n",
    "## now time to actually construct a model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module): #almost everything in pytorch is an nn.Module its like staples for neural network construction\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.randn(1, dtype=torch.float), requires_grad=True) #start with random weights (this will get adjusted as the model learns) PyTorch loves float32 by default\n",
    "        self.bias = nn.Parameter(torch.randn(1, dtype=torch.float), requires_grad=True) #same as above, but for the bias\n",
    "        \n",
    "        #forward defines the computation in the model\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: #x is the input data (e.g. training/testing features)\n",
    "        return self.weights * x + self.bias #this is y = mx + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python classes are how neural networks seem to be build so that everything can be accessed like an object.\n",
    "pytorch has ~4 ish essential modules to create any kinda neural network i can think of:\n",
    "- torch.nn => contains all of the building blocks for computational graphs (essentially a series of computations executed in a particular way)\n",
    "- torch.nn.Parameter => stores tensors that can be used with nn.Module. if requires_grad=True gradients (used for updating model parameters via **gradient descent**) are calculated automatically, this is often reffered to as \"autograd\".\n",
    "- torch.nn.Module => the base class for all neural network modules, all the building blocks for neural networks are subclasses. if youre building a neural network in Pytorch, your models should subclass nn.Module. requires a forward() methid to be implemented.\n",
    "- torch.optim => contains various optimization algorithms (these tell the model parameters stored in nn.Parameter how to best change to improve gradient descent and in turn reduce the loss).\n",
    "- def forward() => all nn.Module subclasses require a forward() method, this defines the computation that will take place on the data passed to the particular nn.Module (e.g. the linear regression formula above).\n",
    "\n",
    "everything comes from torch.nn\n",
    "- nn.Module => blocks to build with\n",
    "- nn.Parameter => smaller parameters like weights and biases (put these together to make nn.Module(s))\n",
    "-forward() => tells the larger blocks how to make calulations on inputs (tensors full of data) within nn.Module(s)\n",
    "- torch.optim => contains optimization methods on how to improve the parameters within nn.Parameter to better represent the input data\n",
    "\n",
    "𓆝 𓆟 𓆞 𓆝 𓆟 𓆝 𓆟 𓆞 𓆝 𓆟 𓆝 𓆟 𓆞 𓆝 𓆟 𓆝 𓆟 𓆞 𓆝 𓆟\n",
    "\n",
    "now its time to create an instance of the model and check it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set manual seed since nn.Parameter are randomly initialized\n",
    "torch.manual_seed(42)\n",
    "\n",
    "##create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "#check the nn.Parameter(s) within the nn.Module subclass i created\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#list the named parameters (what the model contains)\n",
    "model_0.state_dict() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### state_dict(*, prefix: str = '', keep_vars: bool = False) → Dict[str, Any]\n",
    "Returns a dictionary containing references to the whole state of the module.\n",
    "\n",
    "Both parameters and persistent buffers (e.g. running averages) are included. Keys are corresponding parameter and buffer names. Parameters and buffers set to None are not included.\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "The returned object is a shallow copy. It contains references to the module’s parameters and buffers.\n",
    "\n",
    "**WARNING**\n",
    "\n",
    "Currently state_dict() also accepts positional arguments for destination, prefix and keep_vars in order. However, this is being deprecated and keyword arguments will be enforced in future releases.\n",
    "\n",
    "**WARNING**\n",
    "\n",
    "Please avoid the use of argument destination as it is not designed for end-users.\n",
    "\n",
    "**Parameters:**\n",
    "- destination (dict, optional) – If provided, the state of module will be updated into the dict and the same object is returned. Otherwise, an OrderedDict will be created and returned. Default: None.\n",
    "\n",
    "- prefix (str, optional) – a prefix added to parameter and buffer names to compose the keys in state_dict. Default: ''.\n",
    "\n",
    "- keep_vars (bool, optional) – by default the Tensor s returned in the state dict are detached from autograd. If it’s set to True, detaching will not be performed. Default: False.\n",
    "\n",
    "- Returns:\n",
    "a dictionary containing a whole state of the module\n",
    "\n",
    "- Return type:\n",
    "dict\n",
    "\n",
    "Example:\n",
    "```\n",
    ">>> module.state_dict().keys()\n",
    "['bias', 'weight']\n",
    "``` - these are from pytorch docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to check the model i can pass it the test data X_test to see how closely it predicts y_test.\n",
    "when i pass data to a model itll go through the models forward() method and produce a result using the computation defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make predictions with model\n",
    "with torch.inference_mode(): \n",
    "    y_preds = model_0(X_test)\n",
    "#Note: in older PyTorch code you might also see torch.no_grad()\n",
    "#with torch.no_grad():\n",
    "#   y_preds = model_0(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE_MODE\n",
    "CLASStorch.inference_mode(mode=True)[SOURCE]\n",
    "Context-manager that enables or disables inference mode\n",
    "\n",
    "InferenceMode is a new context manager analogous to no_grad to be used when you are certain your operations will have no interactions with autograd (e.g., model training). Code run under this mode gets better performance by disabling view tracking and version counter bumps. Note that unlike some other mechanisms that locally enable or disable grad, entering inference_mode also disables to forward-mode AD.\n",
    "\n",
    "This context manager is thread local; it will not affect computation in other threads.\n",
    "\n",
    "Also functions as a decorator. (Make sure to instantiate with parenthesis.)\n",
    "\n",
    "NOTE\n",
    "\n",
    "Inference mode is one of several mechanisms that can enable or disable gradients locally see Locally disabling gradient computation for more information on how they compare.\n",
    "\n",
    "Parameters:\n",
    "mode (bool) – Flag whether to enable or disable inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inference mode is a context manager that is built into pytorch to use when a model is making predictions.\n",
    "inference mode turns off a bunch of things like gradient tracking to make forward passes (running data through the forward() method) faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of testing samples: 10\n",
      "Number of testing samples: 10\n",
      "Predicted values:\n",
      " tensor([[0.3982],\n",
      "        [0.4049],\n",
      "        [0.4116],\n",
      "        [0.4184],\n",
      "        [0.4251],\n",
      "        [0.4318],\n",
      "        [0.4386],\n",
      "        [0.4453],\n",
      "        [0.4520],\n",
      "        [0.4588]])\n"
     ]
    }
   ],
   "source": [
    "#to check predictions themselves\n",
    "print(f'Number of testing samples: {len(X_test)}')\n",
    "print(f'Number of testing samples: {len(y_preds)}')\n",
    "print(f'Predicted values:\\n {y_preds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions(predictions=y_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is sick i understand whats happening now so im making lots of intellectual progress and iv streamlined my workflow by finally getting adhd meds and being able to focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4618],\n",
       "        [0.4691],\n",
       "        [0.4764],\n",
       "        [0.4836],\n",
       "        [0.4909],\n",
       "        [0.4982],\n",
       "        [0.5054],\n",
       "        [0.5127],\n",
       "        [0.5200],\n",
       "        [0.5272]])"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test - y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these are bad predictions because its first pass\n",
    "\n",
    "right now the model is making predictions using random parameters to make calculations, its basically guessing randomly right now\n",
    "to fix this we can update its internal parameters, the wights and bias values we set randomly using nn.Parameter() and torch.rand() to be something that better represents the data.\n",
    "i can hard code this since i know the default values of weight and bias but i need to learn since i wont know the ideal parameters for a model ahead of time in the field\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating a loss function and optimizer in pytorch\n",
    "for the model to update its parameters on its own it will need a few more parts like an optimizer and a loss function\n",
    "    - loss function => measures how wrong the models predictions are compared to the truth labels, the lower the better with loss. pytorch has many built in loss functions in torch.nn . also in this case y_preds is the predictions, and y_test as the label \n",
    "    - optimizer => tells the model how to update its internal parameters to best lower the loss, pytorch has various optimizer functions in torch.nn\n",
    "\n",
    "\n",
    "depending on the issue the loss and optimizer functions will vary but there are common stuff that works well like stochastic gradient descent and other functions for regression problems(predicting a number)or binary cross enrtopy loss functions for classification problems(predicting one thing or another)\n",
    "\n",
    "for this since im predicting a number im going to use MAE which is torch.nn.L1Loss() in pytorch.\n",
    "\n",
    "Mean absolute error, MAE, or torch.nn.L1Loss measures the absolute difference between two points (predictions and labels) and then takes the mean across all examples\n",
    "\n",
    "for the optimizer im going to use torch.optim.SGD(params, lr) where:\n",
    " - params is the target model params youd like to optimize(the wieghts and bias values randomly set before)\n",
    " - lr is the Learning Rate youd like the optimizer to update the parameters at, higher means the optimizer will try smaller updates (these can sometimes be too large amd the optimizer will fail to work), lower means the optimizer will try smaller updates (these can sometimes be too small and the optimizer will take too long to find the ideal values). the learning rate is considered a hyperparameter (because its set by a machine learning engineer).\n",
    " common starting values for the leaerning rate are 0.01, 0.001, 0.0001, however, these can also be adjusted over time (this is called learning rate scheduling)\n",
    "\n",
    "## How to adjust learning rate\n",
    "torch.optim.lr_scheduler provides several methods to adjust the learning rate based on the number of epochs. torch.optim.lr_scheduler.ReduceLROnPlateau allows dynamic learning rate reducing based on some validation measurements.\n",
    "\n",
    "Learning rate scheduling should be applied after optimizer’s update; e.g., you should write your code this way:\n",
    "\n",
    "Example:\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for input, target in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler.step()\n",
    "Most learning rate schedulers can be called back-to-back (also referred to as chaining schedulers). The result is that each scheduler is applied one after the other on the learning rate obtained by the one preceding it.\n",
    "\n",
    "Example:\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "scheduler1 = ExponentialLR(optimizer, gamma=0.9)\n",
    "scheduler2 = MultiStepLR(optimizer, milestones=[30,80], gamma=0.1)\n",
    "\n",
    "for epoch in range(20):\n",
    "    for input, target in dataset:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input)\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    scheduler1.step()\n",
    "    scheduler2.step()\n",
    "In many places in the documentation, we will use the following template to refer to schedulers algorithms.\n",
    "\n",
    ">>> scheduler = ...\n",
    ">>> for epoch in range(100):\n",
    ">>>     train(...)\n",
    ">>>     validate(...)\n",
    ">>>     scheduler.step()\n",
    "WARNING\n",
    "\n",
    "Prior to PyTorch 1.1.0, the learning rate scheduler was expected to be called before the optimizer’s update; 1.1.0 changed this behavior in a BC-breaking way. If you use the learning rate scheduler (calling scheduler.step()) before the optimizer’s update (calling optimizer.step()), this will skip the first value of the learning rate schedule. If you are unable to reproduce results after upgrading to PyTorch 1.1.0, please check if you are calling scheduler.step() at the wrong time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the loss function\n",
    "loss_fn = nn.L1Loss() #MAE loss is same as L1Loss\n",
    "\n",
    "#create the optimizer \n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.001) #parameters of target model to optimize and the learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now that i have established the loss and optimizer functions by assigning them to the variables for later use, i need to create a training loop and a testing loop to have the model go through training data and learn the relationship between features and labels\n",
    "the testing loop goes through the testing data and evaluates how good the patterns are that the model learn on the training data (the model never sees the testing data during training)\n",
    "Each of these is called a loop because we want our midel to look (loop through) at each sample in each dataset\n",
    "to create these im gonna write a for loop:\n",
    "- forward pass => model goes thorugh all of training data once, performing its forward() function calculations model(x_train)\n",
    "- calculate the loss => the models outputs or predictions are compare to the ground truth and evaluated to see how wrong they are loss = loss_fn(y_pred, y_train)\n",
    "- zero gradients => the optimizers gradients are set to zero (they are accumulated by default) so they can be recalculated for the specific training step optimizer.zero_grad()\n",
    "- perform backpropagation on the loss => computes the gradient of the loss with respect for every parameter to be updated (each parameter with requires_grad=True) this is called backpropagation loss.backward()\n",
    "- update the optimizer => update the parameters with requires_grad=True with respect to find the los gradients in order to improve them optimizer.step()\n",
    "\n",
    "\n",
    "a testing loop includes:\n",
    "- forward pass => the model goes through all of the training data once, performing its forward() function calculations model(x_test)\n",
    "- calculate the loss => the models outputs (predictions) are compared to the ground truth and evaluated to see how wrong they are loss = loss_fn(y_pred, y_test)\n",
    "- calculate evaluation metrics => alongside the loss value sometimes its neccesary to calculate other evaluation metrics such as accuracy on the test set.\n",
    "\n",
    "the testing loop doesnt contain backpropagation (loss.backward()) or stepping the optimizer (optimizer.step()), this is because no parameters in the model are being changed during testing, theyve already been calculated. for testing, i am only interested in the output of the forward pass through the model.\n",
    "an epoch is a forward pass through the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | MAE Train Loss: 0.02092517539858818 | MAE Test Loss: 0.0488395169377327 \n",
      "Epoch: 10 | MAE Train Loss: 0.020581405609846115 | MAE Test Loss: 0.04808080196380615 \n",
      "Epoch: 20 | MAE Train Loss: 0.020238537341356277 | MAE Test Loss: 0.047253452241420746 \n",
      "Epoch: 30 | MAE Train Loss: 0.01989474520087242 | MAE Test Loss: 0.04642607644200325 \n",
      "Epoch: 40 | MAE Train Loss: 0.019551148638129234 | MAE Test Loss: 0.0456673689186573 \n",
      "Epoch: 50 | MAE Train Loss: 0.019208211451768875 | MAE Test Loss: 0.04483998939394951 \n",
      "Epoch: 60 | MAE Train Loss: 0.018864300101995468 | MAE Test Loss: 0.044012635946273804 \n",
      "Epoch: 70 | MAE Train Loss: 0.018520886078476906 | MAE Test Loss: 0.04325391724705696 \n",
      "Epoch: 80 | MAE Train Loss: 0.01817776821553707 | MAE Test Loss: 0.04242655634880066 \n",
      "Epoch: 90 | MAE Train Loss: 0.01783386431634426 | MAE Test Loss: 0.04159919545054436 \n",
      "Epoch: 100 | MAE Train Loss: 0.017490629106760025 | MAE Test Loss: 0.04084048420190811 \n",
      "Epoch: 110 | MAE Train Loss: 0.017147328704595566 | MAE Test Loss: 0.040013112127780914 \n",
      "Epoch: 120 | MAE Train Loss: 0.016803428530693054 | MAE Test Loss: 0.03918575122952461 \n",
      "Epoch: 130 | MAE Train Loss: 0.016460372135043144 | MAE Test Loss: 0.03842703625559807 \n",
      "Epoch: 140 | MAE Train Loss: 0.016116898506879807 | MAE Test Loss: 0.03759966418147087 \n",
      "Epoch: 150 | MAE Train Loss: 0.015772998332977295 | MAE Test Loss: 0.036772288382053375 \n",
      "Epoch: 160 | MAE Train Loss: 0.01543011236935854 | MAE Test Loss: 0.03601359575986862 \n",
      "Epoch: 170 | MAE Train Loss: 0.015086461789906025 | MAE Test Loss: 0.03518623113632202 \n",
      "Epoch: 180 | MAE Train Loss: 0.014742719009518623 | MAE Test Loss: 0.03442750498652458 \n",
      "Epoch: 190 | MAE Train Loss: 0.01439985167235136 | MAE Test Loss: 0.03360014408826828 \n",
      "Epoch: 200 | MAE Train Loss: 0.014056026935577393 | MAE Test Loss: 0.03277278691530228 \n",
      "Epoch: 210 | MAE Train Loss: 0.013712461106479168 | MAE Test Loss: 0.03201407939195633 \n",
      "Epoch: 220 | MAE Train Loss: 0.013369495049118996 | MAE Test Loss: 0.031186699867248535 \n",
      "Epoch: 230 | MAE Train Loss: 0.01302559394389391 | MAE Test Loss: 0.030359338968992233 \n",
      "Epoch: 240 | MAE Train Loss: 0.012682202272117138 | MAE Test Loss: 0.029600614681839943 \n",
      "Epoch: 250 | MAE Train Loss: 0.012339058332145214 | MAE Test Loss: 0.028773266822099686 \n",
      "Epoch: 260 | MAE Train Loss: 0.011995161883533001 | MAE Test Loss: 0.02794589474797249 \n",
      "Epoch: 270 | MAE Train Loss: 0.011651947163045406 | MAE Test Loss: 0.02718718722462654 \n",
      "Epoch: 280 | MAE Train Loss: 0.011308629997074604 | MAE Test Loss: 0.026359815150499344 \n",
      "Epoch: 290 | MAE Train Loss: 0.010964717715978622 | MAE Test Loss: 0.025532448664307594 \n",
      "Epoch: 300 | MAE Train Loss: 0.010621683672070503 | MAE Test Loss: 0.0247737355530262 \n",
      "Epoch: 310 | MAE Train Loss: 0.0102781867608428 | MAE Test Loss: 0.023946374654769897 \n",
      "Epoch: 320 | MAE Train Loss: 0.009934292174875736 | MAE Test Loss: 0.023187648504972458 \n",
      "Epoch: 330 | MAE Train Loss: 0.009591430425643921 | MAE Test Loss: 0.0223603006452322 \n",
      "Epoch: 340 | MAE Train Loss: 0.009247747249901295 | MAE Test Loss: 0.021532922983169556 \n",
      "Epoch: 350 | MAE Train Loss: 0.00890403799712658 | MAE Test Loss: 0.020774226635694504 \n",
      "Epoch: 360 | MAE Train Loss: 0.008561170659959316 | MAE Test Loss: 0.019946854561567307 \n",
      "Epoch: 370 | MAE Train Loss: 0.00821731798350811 | MAE Test Loss: 0.01911948248744011 \n",
      "Epoch: 380 | MAE Train Loss: 0.0078737773001194 | MAE Test Loss: 0.018360769376158714 \n",
      "Epoch: 390 | MAE Train Loss: 0.007530786097049713 | MAE Test Loss: 0.01753341034054756 \n",
      "Epoch: 400 | MAE Train Loss: 0.007186878472566605 | MAE Test Loss: 0.01670604944229126 \n",
      "Epoch: 410 | MAE Train Loss: 0.006843519862741232 | MAE Test Loss: 0.01594732329249382 \n",
      "Epoch: 420 | MAE Train Loss: 0.0065003493800759315 | MAE Test Loss: 0.015119964256882668 \n",
      "Epoch: 430 | MAE Train Loss: 0.0061564454808831215 | MAE Test Loss: 0.014292603358626366 \n",
      "Epoch: 440 | MAE Train Loss: 0.005813260562717915 | MAE Test Loss: 0.013533895835280418 \n",
      "Epoch: 450 | MAE Train Loss: 0.005469911731779575 | MAE Test Loss: 0.012706518173217773 \n",
      "Epoch: 460 | MAE Train Loss: 0.005126009229570627 | MAE Test Loss: 0.011879158206284046 \n",
      "Epoch: 470 | MAE Train Loss: 0.004782738629728556 | MAE Test Loss: 0.011086148209869862 \n",
      "Epoch: 480 | MAE Train Loss: 0.004439300857484341 | MAE Test Loss: 0.010327416472136974 \n",
      "Epoch: 490 | MAE Train Loss: 0.004096209071576595 | MAE Test Loss: 0.0095000509172678 \n",
      "Epoch: 500 | MAE Train Loss: 0.00375230610370636 | MAE Test Loss: 0.008672690019011497 \n",
      "Epoch: 510 | MAE Train Loss: 0.003409049706533551 | MAE Test Loss: 0.007913952693343163 \n",
      "Epoch: 520 | MAE Train Loss: 0.0030657723546028137 | MAE Test Loss: 0.007086604833602905 \n",
      "Epoch: 530 | MAE Train Loss: 0.0027218698523938656 | MAE Test Loss: 0.006259244866669178 \n",
      "Epoch: 540 | MAE Train Loss: 0.0023787878453731537 | MAE Test Loss: 0.005500525236129761 \n",
      "Epoch: 550 | MAE Train Loss: 0.002035339130088687 | MAE Test Loss: 0.004673170857131481 \n",
      "Epoch: 560 | MAE Train Loss: 0.0016914367442950606 | MAE Test Loss: 0.0038457990158349276 \n",
      "Epoch: 570 | MAE Train Loss: 0.0013485305244103074 | MAE Test Loss: 0.0030870854388922453 \n",
      "Epoch: 580 | MAE Train Loss: 0.0010049022966995835 | MAE Test Loss: 0.002259713364765048 \n",
      "Epoch: 590 | MAE Train Loss: 0.000661638390738517 | MAE Test Loss: 0.0014667033683508635 \n",
      "Epoch: 600 | MAE Train Loss: 0.0003177322505507618 | MAE Test Loss: 0.0006393313524313271 \n",
      "Epoch: 610 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 620 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 630 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 640 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 650 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 660 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 670 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 680 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 690 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 700 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 710 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 720 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 730 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 740 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 750 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 760 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 770 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 780 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 790 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 800 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 810 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 820 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 830 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 840 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 850 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 860 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 870 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 880 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 890 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 900 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 910 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 920 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 930 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 940 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 950 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 960 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 970 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 980 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 990 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1000 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1010 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1020 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1030 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1040 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1050 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1060 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1070 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1080 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1090 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1100 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1110 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1120 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1130 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1140 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1150 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1160 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1170 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1180 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1190 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1200 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1210 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1220 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1230 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1240 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1250 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1260 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1270 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1280 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1290 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1300 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1310 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1320 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1330 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1340 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1350 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1360 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1370 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1380 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1390 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1400 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1410 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1420 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1430 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1440 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1450 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1460 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1470 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1480 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1490 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1500 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1510 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1520 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1530 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1540 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1550 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1560 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1570 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1580 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1590 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1600 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1610 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1620 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1630 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1640 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1650 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1660 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1670 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1680 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1690 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1700 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1710 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1720 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1730 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1740 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1750 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1760 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1770 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1780 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1790 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1800 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1810 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1820 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1830 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1840 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1850 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1860 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1870 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1880 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1890 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1900 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1910 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1920 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1930 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1940 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1950 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1960 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1970 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1980 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 1990 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2000 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2010 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2020 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2030 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2040 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2050 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2060 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2070 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2080 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2090 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2100 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2110 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2120 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2130 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2140 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2150 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2160 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2170 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2180 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2190 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2200 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2210 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2220 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2230 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2240 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2250 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2260 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2270 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2280 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2290 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2300 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2310 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2320 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2330 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2340 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2350 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2360 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2370 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2380 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2390 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2400 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2410 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2420 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2430 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2440 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2450 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2460 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2470 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2480 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2490 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2500 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2510 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2520 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2530 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2540 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2550 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2560 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2570 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2580 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2590 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2600 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2610 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2620 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2630 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2640 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2650 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2660 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2670 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2680 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2690 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2700 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2710 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2720 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2730 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2740 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2750 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2760 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2770 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2780 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2790 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2800 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2810 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2820 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2830 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2840 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2850 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2860 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2870 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2880 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2890 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2900 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2910 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2920 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2930 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2940 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2950 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2960 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2970 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2980 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 2990 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3000 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3010 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3020 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3030 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3040 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3050 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3060 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3070 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3080 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3090 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3100 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3110 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3120 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3130 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3140 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3150 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3160 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3170 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3180 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3190 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3200 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3210 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3220 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3230 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3240 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3250 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3260 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3270 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3280 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3290 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3300 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3310 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3320 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3330 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3340 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3350 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3360 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3370 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3380 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3390 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3400 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3410 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3420 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3430 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3440 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3450 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3460 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3470 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3480 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3490 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3500 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3510 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3520 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3530 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3540 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3550 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3560 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3570 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3580 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3590 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3600 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3610 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3620 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3630 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3640 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3650 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3660 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3670 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3680 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3690 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3700 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3710 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3720 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3730 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3740 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3750 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3760 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3770 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3780 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3790 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3800 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3810 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3820 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3830 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3840 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3850 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3860 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3870 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3880 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3890 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3900 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3910 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3920 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3930 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3940 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3950 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3960 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3970 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3980 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 3990 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4000 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4010 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4020 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4030 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4040 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4050 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4060 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4070 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4080 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4090 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4100 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4110 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4120 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4130 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4140 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4150 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4160 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4170 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4180 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4190 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4200 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4210 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4220 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4230 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4240 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4250 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4260 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4270 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4280 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4290 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4300 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4310 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4320 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4330 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4340 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4350 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4360 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4370 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4380 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4390 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4400 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4410 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4420 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4430 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4440 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4450 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4460 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4470 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4480 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4490 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4500 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4510 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4520 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4530 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4540 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4550 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4560 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4570 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4580 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4590 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4600 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4610 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4620 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4630 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4640 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4650 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4660 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4670 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4680 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4690 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4700 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4710 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4720 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4730 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4740 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4750 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4760 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4770 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4780 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4790 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4800 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4810 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4820 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4830 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4840 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4850 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4860 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4870 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4880 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4890 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4900 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4910 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4920 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4930 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4940 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4950 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4960 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4970 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4980 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 4990 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5000 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5010 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5020 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5030 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5040 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5050 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5060 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5070 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5080 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5090 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5100 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5110 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5120 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5130 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5140 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5150 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5160 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5170 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5180 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5190 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5200 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5210 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5220 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5230 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5240 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5250 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5260 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5270 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5280 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5290 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5300 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5310 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5320 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5330 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5340 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5350 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5360 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5370 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5380 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5390 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5400 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5410 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5420 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5430 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5440 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5450 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5460 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5470 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5480 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5490 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5500 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5510 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5520 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5530 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5540 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5550 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5560 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5570 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5580 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5590 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5600 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5610 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5620 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5630 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5640 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5650 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5660 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5670 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5680 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5690 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5700 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5710 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5720 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5730 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5740 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5750 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5760 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5770 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5780 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5790 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5800 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5810 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5820 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5830 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5840 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5850 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5860 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5870 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5880 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5890 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5900 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5910 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5920 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5930 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5940 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5950 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5960 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5970 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5980 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 5990 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6000 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6010 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6020 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6030 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6040 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6050 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6060 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6070 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6080 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6090 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6100 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6110 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6120 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6130 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6140 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6150 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6160 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6170 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6180 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6190 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6200 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6210 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6220 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6230 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6240 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6250 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6260 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6270 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6280 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6290 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6300 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6310 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6320 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6330 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6340 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6350 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6360 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6370 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6380 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6390 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6400 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6410 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6420 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6430 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6440 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6450 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6460 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6470 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6480 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6490 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6500 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6510 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6520 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6530 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6540 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6550 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6560 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6570 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6580 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6590 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6600 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6610 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6620 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6630 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6640 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6650 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6660 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6670 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6680 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6690 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6700 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6710 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6720 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6730 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6740 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6750 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6760 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6770 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6780 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6790 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6800 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6810 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6820 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6830 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6840 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6850 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6860 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6870 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6880 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6890 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6900 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6910 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6920 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6930 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6940 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6950 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6960 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6970 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6980 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 6990 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7000 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7010 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7020 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7030 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7040 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7050 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7060 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7070 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7080 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7090 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7100 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7110 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7120 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7130 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7140 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7150 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7160 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7170 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7180 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7190 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7200 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7210 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7220 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7230 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7240 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7250 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7260 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7270 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7280 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7290 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7300 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7310 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7320 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7330 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7340 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7350 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7360 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7370 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7380 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7390 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7400 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7410 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7420 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7430 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7440 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7450 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7460 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7470 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7480 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7490 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7500 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7510 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7520 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7530 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7540 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7550 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7560 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7570 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7580 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7590 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7600 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7610 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7620 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7630 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7640 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7650 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7660 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7670 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7680 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7690 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7700 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7710 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7720 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7730 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7740 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7750 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7760 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7770 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7780 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7790 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7800 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7810 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7820 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7830 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7840 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7850 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7860 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7870 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7880 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7890 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7900 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7910 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7920 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7930 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7940 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7950 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7960 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7970 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7980 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 7990 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8000 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8010 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8020 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8030 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8040 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8050 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8060 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8070 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8080 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8090 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8100 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8110 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8120 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8130 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8140 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8150 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8160 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8170 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8180 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8190 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8200 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8210 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8220 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8230 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8240 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8250 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8260 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8270 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8280 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8290 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8300 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8310 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8320 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8330 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8340 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8350 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8360 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8370 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8380 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8390 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8400 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8410 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8420 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8430 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8440 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8450 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8460 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8470 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8480 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8490 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8500 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8510 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8520 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8530 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8540 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8550 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8560 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8570 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8580 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8590 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8600 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8610 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8620 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8630 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8640 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8650 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8660 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8670 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8680 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8690 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8700 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8710 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8720 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8730 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8740 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8750 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8760 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8770 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8780 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8790 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8800 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8810 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8820 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8830 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8840 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8850 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8860 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8870 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8880 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8890 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8900 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8910 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8920 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8930 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8940 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8950 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8960 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8970 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8980 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 8990 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9000 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9010 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9020 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9030 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9040 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9050 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9060 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9070 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9080 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9090 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9100 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9110 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9120 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9130 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9140 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9150 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9160 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9170 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9180 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9190 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9200 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9210 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9220 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9230 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9240 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9250 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9260 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9270 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9280 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9290 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9300 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9310 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9320 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9330 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9340 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9350 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9360 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9370 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9380 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9390 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9400 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9410 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9420 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9430 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9440 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9450 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9460 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9470 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9480 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9490 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9500 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9510 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9520 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9530 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9540 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9550 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9560 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9570 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9580 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9590 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9600 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9610 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9620 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9630 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9640 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9650 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9660 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9670 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9680 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9690 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9700 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9710 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9720 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9730 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9740 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9750 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9760 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9770 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9780 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9790 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9800 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9810 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9820 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9830 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9840 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9850 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9860 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9870 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9880 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9890 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9900 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9910 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9920 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9930 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9940 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9950 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9960 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9970 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9980 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n",
      "Epoch: 9990 | MAE Train Loss: 0.0004444979131221771 | MAE Test Loss: 0.0005780101055279374 \n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs (how many times the model will pass over the training data)\n",
    "epochs = 10000\n",
    "\n",
    "# Create empty loss lists to track values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "\n",
    "    # Put model in training mode (this is the default state of a model)\n",
    "    model_0.train()\n",
    "\n",
    "    # 1. Forward pass on train data using the forward() method inside \n",
    "    y_pred = model_0(X_train)\n",
    "    # print(y_pred)\n",
    "\n",
    "    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "\n",
    "    # 3. Zero grad of the optimizer\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "\n",
    "    # 5. Progress the optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model_0.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass on test data\n",
    "      test_pred = model_0(X_test)\n",
    "\n",
    "      # 2. Caculate loss on test data\n",
    "      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
    "\n",
    "      # Print out what's happening\n",
    "      if epoch % 10 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            test_loss_values.append(test_loss.detach().numpy())\n",
    "            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss go down!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot the loss curves\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "im currently going through a weird matplot lib issue in vscode so its erroring in all my plots\n",
    "(i think, this could always just be i am currently stupid and cant see why till i escape it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model learned the following values for weights and bias:\n",
      "OrderedDict([('weights', tensor([0.6994])), ('bias', tensor([0.2998]))])\n",
      "\n",
      "And the original values for weights and bias are:\n",
      "weights: 0.7, bias: 0.3\n"
     ]
    }
   ],
   "source": [
    "# Find our model's learned parameters\n",
    "print(\"The model learned the following values for weights and bias:\")\n",
    "print(model_0.state_dict())\n",
    "print(\"\\nAnd the original values for weights and bias are:\")\n",
    "print(f\"weights: {weight}, bias: {bias}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
