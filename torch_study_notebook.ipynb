{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# the following is the first notebook entry as i start to learn machine learning myself\n",
    "## hopefully i can learn to do this and keep up a journal of my progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0+cpu'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scalar\n",
    "scalar = torch.tensor(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scalars are 0 dimensional tensors, they have no shape, no length, no dimension, no size\n",
    "they are created the same way as tensors, but with a single number, not a list\n",
    "the dimensions of a tensor a tensor start at 0 and then go to 1, 2, 3, etc \n",
    "with a one dimensional tensor, you can think of it as a list of numbers like [1, 2, 3, 4, 5]\n",
    "for example torch.tensor([1, 2, 3, 4, 5]) is a one dimensional tensor with 5 elements\n",
    "and torch.tensor([1, 2, 3, 4, 5]).shape is torch.Size([5])\n",
    "whereas torch.tensor(7) is a 0 dimensional tensor with 1 element\n",
    "and torch.tensor([1],[2]) is a 2 dimensional tensor with 2 elements\n",
    "and torch.tensor([1],[2]).shape is torch.Size([2, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scalar dimensions:  0\n"
     ]
    }
   ],
   "source": [
    "#to check the dimensions of a tensor (number of rows and columns)\n",
    "scalar.ndim\n",
    "#for clarity\n",
    "print(\"scalar dimensions: \", scalar.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to get the number from the tensor\n",
    "scalar.item()\n",
    "#this will convert the value inside of the tensor to a python integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a vector is a single dimensional tensor that can have as many values as you want\n",
    "vector = torch.tensor([7,7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the dimensions of the vector\n",
    "vector.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the number of dimensions will match the number of brackets in the array\n",
    "([]) = 1D array (vector), ([[]]) = 2D array, ([[[ ]]]) = 3D array\n",
    "the shape of a tensor is how the elements inside the tensor is arranged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the tensor is:  torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "#this is how you check the shape\n",
    "print(\"The shape of the tensor is: \", vector.shape)\n",
    "#the size([2])means our vector has a shape of [2] this is because of the 2 elements in the vector\n",
    "#([7,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#this is a matrix\n",
    "matrix = torch.tensor([[7,8],\n",
    "                      [9,10]])\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.ndim\n",
    "#its a 2D Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape\n",
    "#the shape is 2 rows and 2 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up until now they have all been gone over as tensors\n",
    "but its really like\n",
    "0D tensor = scalar, 1D tensor = vector, 2D tensor = matrix\n",
    "3D tensor = cube of numbers\n",
    "anything else would be a higher order tensor (4D, 5D, etc)\n",
    "like a hypercube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [3, 6, 9],\n",
       "         [2, 4, 5]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is a tensor\n",
    "TENSOR = torch.tensor([[[1,2,3],\n",
    "                        [3,6,9],\n",
    "                        [2,4,5]]])\n",
    "TENSOR\n",
    "#tensors can represent almost anything\n",
    "#the above tensor is a representation of sales numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "TENSOR.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.shape\n",
    "#this is a bit confusing as it outputs ([1,3,3])\n",
    "#but its a 3 dimenrsional tensor\n",
    "#the dimensions go outter to inner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nin conclusion, i have learned about the following:\\nscalar = single number and its dimension is 0\\nvector = a number with dirextion(like wind speed with direction) its dimension is 1\\nmatrix = a 2d array of numbers its dimension is 2\\ntensor = a n-d array of numbers its dimension is n\\n'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "in conclusion, i have learned about the following:\n",
    "scalar = single number and its dimension is 0\n",
    "vector = a number with dirextion(like wind speed with direction) its dimension is 1\n",
    "matrix = a 2d array of numbers its dimension is 2\n",
    "tensor = a n-d array of numbers its dimension is n\n",
    "\"\"\"\n",
    "#ive learned i can use docstrings randomly for notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.8437, 0.4264, 0.2244, 0.0919],\n",
       "         [0.4324, 0.7299, 0.6532, 0.8335],\n",
       "         [0.2882, 0.0357, 0.0098, 0.3863]]),\n",
       " torch.float32,\n",
       " 2,\n",
       " torch.Size([3, 4]))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to create a tensor of random numbers\n",
    "random_tensor = torch.rand(size=(3,4))\n",
    "random_tensor, random_tensor.dtype, random_tensor.ndim, random_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([244, 244, 3]), 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this can be used to make random tensors in whatever size you want\n",
    "#for example you could use torch.random(244,244,3)\n",
    "#this wuold follow the common shape of an image\n",
    "random_image_size_tensor = torch.rand(size=(244,244,3))\n",
    "random_image_size_tensor.shape, random_image_size_tensor.ndim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hey copilot do i need to specify the size = key?\n",
    "- copilot: no, you can just do torch.rand((244,244,3))\n",
    "- why the double parenthesis?\n",
    "- copilot: because it's a tuple\n",
    "- so rand takes a tuple as an argument?\n",
    "- copilot: yes\n",
    "- will rand(244,244,3) work?\n",
    "- copilot: no, it will throw an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to create a tensor full of zeros\n",
    "zeros = torch.zeros(3,4)\n",
    "zeros, zeros.dtype\n",
    "#this can be used to create a tensor of any shape\n",
    "#and the zeros can be accessed and filled with values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to create a tensor full of ones\n",
    "ones = torch.ones(3,4)\n",
    "ones, ones.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to create a range of values\n",
    "zero_to_ten = torch.arange(start=0, end=10, step=1)\n",
    "\n",
    "zero_to_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the pytorch arange (not arrange) function\n",
    "it is used to create a range of values with a set interval\n",
    "the start is the first value in the range\n",
    "the end is the last value in the range\n",
    "the step is the interval between each value in the range\n",
    "its important to remember it returns a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hey copilot is like everything in torch just about tensors?\n",
    "- copilot: yes, tensors are the main data structure in torch\n",
    "- copilot: tensors are like numpy arrays, but they can also be used on a GPU to accelerate computing\n",
    "- copilot: tensors are also optimized for automatic differentiation\n",
    "- copilot: tensors are also the main data structure for neural networks in pytorch\n",
    "- thanks copilot\n",
    "- copilot: no problem\n",
    "- copilot: I'm here to help\n",
    "- is shape a very important thing in torch?\n",
    "- copilot: yes, shape is very important in torch\n",
    "- so in the same way that i would need torch.zeros and torch.ones\n",
    "to create a tensor of zeros and ones of any size from the jump\n",
    "i would need torch.zeros_like and torch.ones_like\n",
    "to create a tensor of ones or zeros in the same shape\n",
    "as some other data im working with?\n",
    "- copilot: yes, that's correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#to create a tensor of zeros or ones in the same shape as some other data\n",
    "ten_zeros = torch.zeros_like(zero_to_ten)\n",
    "ten_zeros "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- copilot why are there so many data types in pytorch?\n",
    "- copilot: because it's a dynamic language\n",
    "- copilot: and it's a language that's designed to be used in a dynamic way\n",
    "- is it bc some of the data types are for the gpu and some are for the cpu\n",
    "- copilot: no, it's because some of the data types are for the gpu and some are for the cpu\n",
    "- that is literally what i asked you\n",
    "- copilot: i'm not sure what you mean by that\n",
    "- -.-\n",
    "- what is the most common data type?\n",
    "- copilot: the most common data type is the float\n",
    "- so like a float32?\n",
    "- copilot: yes\n",
    "- what is the difference between a float and a float32?\n",
    "- copilot: a float is a floating point number, and a float32 is a floating point number with 32 bits\n",
    "- to create a tensor with a specific datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.float32, device(type='cpu'), False)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=None, #the default arg is none, which is automatically a float32\n",
    "                               device=None, #the default arg is none, which is automatically a cpu\n",
    "                               requires_grad=False) #the default arg is false, which means that the tensor will not be used in a gradient computation, if true then operations on the tensor will be recorded\n",
    "\n",
    "float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device, float_32_tensor.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=torch.float16) #torch.half would also work\n",
    "\n",
    "float_16_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4767, 0.0268, 0.7756, 0.1756],\n",
      "        [0.0338, 0.4729, 0.4134, 0.6030],\n",
      "        [0.1547, 0.5538, 0.3529, 0.2361]])\n",
      "shape of some_tensor: torch.Size([3, 4])\n",
      "dtype of some_tensor: torch.float32\n",
      "device of some_tensor: cpu\n"
     ]
    }
   ],
   "source": [
    "some_tensor = torch.rand(3, 4)\n",
    "\n",
    "print(some_tensor)\n",
    "print(f'shape of some_tensor: {some_tensor.shape}')\n",
    "print(f'dtype of some_tensor: {some_tensor.dtype}')\n",
    "print(f'device of some_tensor: {some_tensor.device}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 2, 3]), tensor([11, 12, 13]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#math works on tensors\n",
    "#to create a tensor and add to it\n",
    "tensor = torch.tensor([1,2,3])\n",
    "tensor, tensor + 10\n",
    "#this will add 10 to each element in the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to multiply\n",
    "tensor * 10\n",
    "#this will multiply each element by 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensors dont change until they have been reassigned\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9, -8, -7])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor - 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tensor + 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.multiply(tensor, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) * tensor([1, 2, 3])\n",
      "Equals: tensor([1, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "# Element-wise multiplication (each element multiplies its equivalent, index 0->0, 1->1, 2->2)\n",
    "print(tensor, \"*\", tensor)\n",
    "print(\"Equals:\", tensor * tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), 1)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing matrix multplication\n",
    "tensor = torch.tensor([1,2,3]) #one dimensional tensor, or vector wirh a shape of [3]\n",
    "tensor.shape, tensor.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the difference between elemeent wise multiplication\n",
    "and matrix multiplication is the addition of value\n",
    "for tensor i can think about element wise multiplication as\n",
    "[1*1, 2*2, 3*3] = [1,4,9] tensor * tensor\n",
    "and then i can think of a matrix multiplication as [1*1, 2*2, 3*3] = [14]\n",
    "tensor.matmula(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 4, 9]), tensor(14))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#elementwise, matmulwise\n",
    "tensor * tensor, torch.matmul(tensor, tensor)\n",
    "#this will be useful to quickly do math even graphics like rotating an ascii cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14), tensor(19))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#its either in python itself or pytorch that the @ \n",
    "#is the symbol for matrix multiplicaion\n",
    "tensor @ tensor, tensor @ torch.tensor([1,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in the past algo has told me not to waste time with writing my own matrix multiplication functions\n",
    "this is not to ignore that but to at least know whats under the hood so im not blindly using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#by hand implementation\n",
    "value = 0\n",
    "for i in range(len(tensor)):\n",
    "    value += tensor[i] * tensor[i]\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#this is obviously much faster\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 10\u001b[0m\n\u001b[0;32m      2\u001b[0m tensor_A \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m],\n\u001b[0;32m      3\u001b[0m                          [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m],\n\u001b[0;32m      4\u001b[0m                          [\u001b[39m5\u001b[39m, \u001b[39m6\u001b[39m]], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m      6\u001b[0m tensor_B \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m7\u001b[39m, \u001b[39m10\u001b[39m],\n\u001b[0;32m      7\u001b[0m                          [\u001b[39m8\u001b[39m, \u001b[39m11\u001b[39m], \n\u001b[0;32m      8\u001b[0m                          [\u001b[39m9\u001b[39m, \u001b[39m12\u001b[39m]], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m---> 10\u001b[0m torch\u001b[39m.\u001b[39;49mmatmul(tensor_A, tensor_B) \u001b[39m# (this will error)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "# Shapes need to be in the right way  \n",
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)\n",
    "\n",
    "torch.matmul(tensor_A, tensor_B) # (this will error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this didnt seem obvious to me at first and now i understand\n",
    "what algo and chris meant by touch the tensor and see the shape\n",
    "its becoming clear i coulndt progress without proper pytorch knowlege\n",
    "\n",
    "at first to the eye intuitively these look like something should be able\n",
    "to be done with them and there prolly is but its not what i am trying to\n",
    "do with them right now, the shape is identical, and thats the issue\n",
    "in the case of matrix multiplication the shape has to have the same inner\n",
    "multiplacitively speaking, so [3,2] @ [3,2] wont work but\n",
    "[anything, 2] @ [2, anything] would and if there are errors like this\n",
    "in the future i can just touch the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 2]), torch.Size([3, 2]))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_A.shape, tensor_B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 7., 10.],\n",
      "        [ 8., 11.],\n",
      "        [ 9., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# View tensor_A and tensor_B\n",
    "print(tensor_A)\n",
    "print(tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# View tensor_A and tensor_B.T\n",
    "print(tensor_A)\n",
    "print(tensor_B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n",
      "\n",
      "New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n",
      "\n",
      "Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) <- inner dimensions match\n",
      "\n",
      "Output:\n",
      "\n",
      "tensor([[ 27.,  30.,  33.],\n",
      "        [ 61.,  68.,  75.],\n",
      "        [ 95., 106., 117.]])\n",
      "\n",
      "Output shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# The operation works when tensor_B is transposed\n",
    "print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\n",
    "print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\n",
    "print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\n",
    "print(\"Output:\\n\")\n",
    "output = torch.matmul(tensor_A, tensor_B.T)\n",
    "print(output) \n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27.,  30.,  33.],\n",
       "        [ 61.,  68.,  75.],\n",
       "        [ 95., 106., 117.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.mm is a shortcut for matmul\n",
    "torch.mm(tensor_A, tensor_B.T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear algebra is supposed to be important in machine learning\n",
    "torch is a library for linear algebra\n",
    "torch is a library for tensor computation\n",
    "tensor is a generalization of vectors and matrices\n",
    "a liner linear function can be accurately represented (i think) \n",
    "as **y = x * A^T + b**\n",
    "where x is the input to the layer,\n",
    "A is the weight matrix, created by the layer, it starts out as random\n",
    "numbers that get adjusted as neural networks lean to better represent\n",
    "patterns in the data\n",
    "T is the transpose of A, because the matrix of weights gets transposed\n",
    "b is the bias vector, created by the layer, it starts out as random\n",
    "y is the output of the layer\n",
    "if this is inaccurate dont sue me im wingin this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([2, 3])\n",
      "\n",
      "output:\n",
      "tensor([[0.9332, 0.8805, 3.0149, 1.5545, 1.8186, 2.0634],\n",
      "        [1.7186, 1.4009, 3.5818, 1.7408, 2.6017, 2.5123]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([2, 6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#trying out a linear layer\n",
    "#since it starts as a random weight matrix, i can make it reproduceable wtih a seed\n",
    "torch.manual_seed(42)\n",
    "#this uses matrix multiplication\n",
    "linear = torch.nn.Linear(in_features=2, #in_features is the number of inputs\n",
    "                         out_features=6)  #out_features is the number of outputs, it describes outter values\n",
    "x = tensor_A\n",
    "output = linear(x)\n",
    "print(f\"input shape: {x.shape}\\n\")\n",
    "print(f\"output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if i were to change the in_features to 3, it would throw an error\n",
    "because the input is a 2d tensor, and the in_features is 3\n",
    "it can be fixed with a transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a tensor\n",
    "x = torch.arange(0, 100, 10) #dont forget that arange is arange(start, end, step)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum:0, maximum:90\n",
      "mean:45.0\n",
      "sum:450\n"
     ]
    }
   ],
   "source": [
    "print(f\"minimum:{x.min()}, maximum:{x.max()}\")\n",
    "# print(F'mean:{x.mean()}) #this will error\n",
    "print(f'mean:{x.type(torch.float32).mean()}') #this operation wont work if i just do {x.mean()}\n",
    "print(f'sum:{x.sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(90), tensor(45.), tensor(450))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the follwing will work the same as above\n",
    "torch.min(x), torch.max(x), torch.mean(x.type(torch.float32)), torch.sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
      "index where max value is 8\n",
      "index where min value is 0\n"
     ]
    }
   ],
   "source": [
    "#to learn indexing i have to create a new tensor\n",
    "tensor = torch.arange(10,100,10)\n",
    "print(tensor)\n",
    "print(f'index where max value is {tensor.argmax()}')\n",
    "print(f'index where min value is {tensor.argmin()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new tensor(prolly gonna do this a ton)\n",
    "tensor = torch.arange(10.,100.,10.)\n",
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a float 16 tensor\n",
    "tensorfloat16 = tensor.type(torch.float16) #type will create a copy of the tensor fed to it through it and convert it to the type specified\n",
    "tensorfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorint8 = tensor16.type(torch.int8)\n",
    "tensorint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a tensor\n",
    "x = torch.arange(1.,8.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#im going to try adding dimension with reshape()\n",
    "x_reshape = x.reshape(1,7)\n",
    "x_reshape, x_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#another way to change the shape of a tensor is to use .view() method\n",
    "z = x.view(1,7)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### quick ref will return to later\n",
    "Assume x is a 2-dimensional tensor (or a matrix). Here are some ways you can index into x:\n",
    "Select a specific element: x[i, j] will select the element at the i-th row and j-th column.\n",
    "Select a specific row or column: x[i, :] will select the i-th row, and x[:, j] will select the j-th column.\n",
    "Select multiple rows or columns: x[1:3, :] will select rows 1 and 2 (note that the end index is exclusive), \n",
    "and x[:, 1:3] will select columns 1 and 2.\n",
    "Select with a step: x[::2, :] will select every other row, starting from row 0.\n",
    "Select with negative indices: x[-1, :] will select the last row. Negative indices count from the end.\n",
    "Use boolean indexing: If you have a tensor of the same shape as x, but with boolean values (i.e., True and False),\n",
    "you can use this as an index. x[boolean_tensor] will select all elements of x where the corresponding value in the boolean tensor is True.\n",
    "Use integer indexing: You can create a tensor of integers to select multiple rows or columns in a specific order or multiple times.\n",
    "x[[1, 2, 1], :] will select the second, third, and second rows of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "z[:,0] = 5\n",
    "z,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack tensors on top of each other\n",
    "x_stacked = torch.stack([x, x, x, x], dim=0) #stack is used to stack tensors on top of each other, which adds a new dimension\n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous tensor:tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "previous shape:torch.Size([1, 7])\n",
      "new tensor:tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "new shape:torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "#to remove all single dimensions from a tensor use torch.squeeze()\n",
    "print(f'previous tensor:{x_reshape}')\n",
    "print(f'previous shape:{x_reshape.shape}')\n",
    "x_squeezed = torch.squeeze(x_reshape) #torch.squeeze() removes all single dimensions from a tensor in other words it removes anythihng that has a size of 1\n",
    "print(f'new tensor:{x_squeezed}')\n",
    "print(f'new shape:{x_squeezed.shape}')\n",
    "#youre squeezing the tensor to only have dimensions over 1, or squeezing out all the 1s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "tensor([[5., 2., 3., 4., 5., 6., 7.]])\n"
     ]
    }
   ],
   "source": [
    "print(x_squeezed)\n",
    "xunsqueezed = x_squeezed.unsqueeze(dim=0) #this adds a dimension to a specific index\n",
    "print(xunsqueezed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i can also rearrange the order of axes values with permute(input, dims)\n",
    "where input gets turned into a view with new dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([244, 244, 3])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#create a tensor with a specific shape\n",
    "x_og = torch.rand(size=(244,244,3))\n",
    "\n",
    "#permute the og tensor to rearrange the axis order\n",
    "x_perm = x_og.permute(2,0,1)\n",
    "\n",
    "print(x_og)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " .-._                                                   _,-,\n",
    "  `._`-._                                           _,-'_,'\n",
    "     `._ `-._                                   _,-' _,'\n",
    "        `._  `-._        __.-----.__        _,-'  _,'\n",
    "           `._   `#===\"\"\"           \"\"\"===#'   _,'\n",
    "              `._/)  ._               _.  (\\_,'\n",
    "               )*'     **.__     __.**     '*( \n",
    "               #  .==..__  \"\"   \"\"  __..==,  # \n",
    "BOO            #   `\"._(_).       .(_)_.\"'   #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# indexing(selecting data from tensors)\n",
    "to select specific data from tensors(like only the first column or second row)\n",
    "to do so i can index like in vanilla python or numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [4, 5, 6],\n",
       "          [7, 8, 9]]]),\n",
       " torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a tensor\n",
    "import torch\n",
    "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indexing the values of a tensor goes: outer dimension -> inner dimension (check brackets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first square bracket\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "second square bracket\n",
      "tensor([1, 2, 3])\n",
      "third square bracket\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "#indexing bracket by bracket\n",
    "print(f'first square bracket\\n{x[0]}')\n",
    "print(f'second square bracket\\n{x[0][0]}')\n",
    "print(f'third square bracket\\n{x[0][0][0]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "on god for real for real, i feel like this is probnobably going to be the most im[port]ant thing i have written so far. it seems like all of the\n",
    "cool stuff ive seen algo do is like really sick indexing, the idea of saying \"really sick indexing\" is making me wanna **play sudoku**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i can also use : to specify \"all values in this dimension\" and then use a comma to add another dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]])\n"
     ]
    }
   ],
   "source": [
    "#get all values of the 0th dimension and the 0 index of the 1st dimension\n",
    "print(x[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 5, 8]])\n"
     ]
    }
   ],
   "source": [
    "#get all the values of the 0th and 1st dimensions but only the index 1 of the 2nd dimension\n",
    "print(x[:, :, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n",
    "x[:, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get index 0 of 0th and 1st dimension and all values of 2nd dimension \n",
    "x[0, 0, :] # same as x[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since numpy is so popular pytorch is made to interface with it well\n",
    "two main methods to make early note of are:\n",
    "- torch.from_numpy(ndarray) - numpy array to pytorch tensor\n",
    "- torch.Tensor.numpy() - pytorch tensor to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#numpy to tensor\n",
    "import numpy as np\n",
    "\n",
    "array = np.arange(1.0, 8.0)\n",
    "tensor = torch.from_numpy(array)\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by default numpy arrays are created with the datatype float64 and if you convert it to a pytorch tensor it will keep the same datatype.\n",
    "however because so many tensors are float32 i can convert the array with type after its been made into a tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 3., 4., 5., 6., 7., 8.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change array keep tensor\n",
    "array = array + 1\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tensor to numpy\n",
    "tensor = torch.ones(7)\n",
    "numpy_tensor = tensor.numpy()\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#change tensor keep the array\n",
    "tensor = tensor + 1\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reproducability is important in machine learning and data science so i need to learn how to use randomness in a reproducable way\n",
    "start with random #'s => tensor operations => improve "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor a: tensor([[0.4691, 0.2155, 0.8268, 0.4587],\n",
      "        [0.9260, 0.5322, 0.4424, 0.4334],\n",
      "        [0.4530, 0.5870, 0.2849, 0.6589]])\n",
      "\n",
      "tensor b: tensor([[0.8266, 0.3321, 0.5549, 0.6163],\n",
      "        [0.4418, 0.5237, 0.4263, 0.6685],\n",
      "        [0.7419, 0.2461, 0.5976, 0.6359]])\n",
      "\n",
      "does tensor a = tensor b anywhere?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create two random tensors\n",
    "randa = torch.rand(3,4)\n",
    "randb = torch.rand(3,4)\n",
    "\n",
    "print(f'tensor a: {randa}\\n')\n",
    "print(f'tensor b: {randb}\\n')\n",
    "print(f'does tensor a = tensor b anywhere?')\n",
    "randa == randb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "these are different even though they both look identical because they are random from a different seed\n",
    "i can use torch.manual_seed(seed) to set the seed to the same place every time so the sensation of random is aligned in that instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "randomtensor: tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "randomtensor2: tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "does randomtensor == randomtensor2? tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#set the random seed\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "randomtensor = torch.rand(3, 4)\n",
    "                                    # its important to set the seed before creating the tensor each time\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "randomtensor2 = torch.rand(3, 4)\n",
    "\n",
    "print(f'randomtensor: {randomtensor}')\n",
    "print(f'randomtensor2: {randomtensor2}')\n",
    "print(f'does randomtensor == randomtensor2? {randomtensor == randomtensor2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣤⣤⣤⣤⣤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⣴⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ you can do all of this stuff on cpu or gpu if you have one⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⣿⠿⠋⠉⠁⠀⠀⠀⠹⢿⡿⣿⡻⠟⠿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⡿⡃⠀⠀⠀⠀⠀⠀⠀⠰⡄⢹⣿⡇⠀⢻⡿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣻⠁⠀⠀⠀⠀⠀⠀⠀⠀⣛⣼⡿⠃⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣶⣿⣿⣷⣶⣦⣤⣀⡀⠀⠀                  running tensors on gpus\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣿⡇⠻⣿⣤⣀⠀⠀⢀⣠⣤⡶⠟⠛⠁⠀⠀⠀⠈⠙⠓⠲⠶⠦⣤⣤⣄⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⠟⠋⢙⣿⣿⣿⣿⣿⣿⣿⣿⣶⣄               lets the computer do all the\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠹⣿⣦⠈⠉⠛⠛⠛⠋⠉⠀⠀⢀⣀⣀⣀⣠⣤⣤⣤⣤⣤⣤⣀⣀⣀⣀⠈⠉⠙⠓⠲⠶⢤⣀⡀⠀⠀⠀⡏⠀⢠⣾⠋⠁⠛⣉⡈⠙⠻⢿⣿⣿              calculations way more better\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠙⣿⠇⠀⢀⣀⣴⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⣦⣄⣀⠀⠉⠉⠛⠛⠃⠀⠀⣿⡄⠀⠀⠀⠀⠀⠀⠀⠙⢿               so that everything is done\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⡾⢃⣤⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣄⡀⠀⠀⠀⠉⢷⣄⠀⠀⠀⠀⠀⠀⠀⢠               faster and more efficient\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣯⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⡀⠀⠀⠙⢷⣄⠀⠀⠀⠀⠀⠀\\n⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⣿⣿⣿⣿⣿⣿⠟⠋⠉⠉⠉⠉⠛⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⠀⠀⠙⠳⣤⣀⠀⠀⣐\\n⠀⠀⠀⠀⠀⠀⠀⠀⣰⢟⣿⣿⣿⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠈⠉⠛⠛⠉\\n⠀⠀⠀⠀⠀⠀⠀⣼⢏⣾⣿⣿⣿⣿⣿⡟⠛⠛⣶⣄⣀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡀⠀⠀⢈⣼⡿\\n⠀⠀⠀⠀⠀⠀⣸⠏⣾⣿⣿⣿⣿⣿⣿⠀⠀⢰⡏⠉⠙⠻⠷⣶⣤⣄⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠛⠋⠉⠛⠻⣿⣿⣿⣿⣿⣄⠀⢿⠁⠀\\n⠀⠀⠀⠀⠀⣴⡏⣸⣿⣿⣿⣿⣿⣿⡏⠀⠀⢸⡇⠀⠀⠀⣠⡿⠃⠈⣽⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠃⠀⠀⠀⠀⠀⠀⠈⢻⣿⣿⣿⣿⣆⠸⣇⠀\\n⠀⠀⠀⠀⣰⡟⢰⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠘⢷⣤⣤⠾⠋⠀⠀⣰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⡆⢹⡀\\n⠀⠀⠀⣰⣿⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣆⠀⠀⠀⠀⠀⠀⠀⣀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠷⠤⣤⣤⣤⣤⣤⣤⣤⣀⣀⣸⣿⣿⣿⣿⣷⡈⢧\\n⠀⠀⠀⣿⡇⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣤⣤⣤⣴⣶⣿⣿⣿⣿⣿⣿⣿⠿⠿⠿⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⢹⡆⠀⠀⠀⢈⣽⠃⠀⢹⣿⣿⣿⣿⣿⡇⠈\\n⠀⠀⠀⣿⡇⠀⠈⢿⣿⣿⣿⣿⠿⠿⢿⣽⡟⣿⣿⣿⣿⣿⣿⡿⠟⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠉⠛⠻⢿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠈⢷⡄⢀⣠⡿⠁⠀⢀⣿⣿⣿⣿⣿⣿⡇⠀                 \\n⠀⠀⠀⣿⢑⣂⣠⣤⣿⣿⡷⢶⣶⣶⠀⠙⣿⢻⣿⡿⠟⠉⠁⠀⠀⠀⠀⢀⣤⣴⣶⣶⣶⣦⣤⣀⠀⠀⠀⠈⠛⢿⣿⣿⣿⣿⣧⠀⠀⠀⠙⠛⠉⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⡇⠀\\n⠀⠀⠀⣿⣾⣿⣿⣏⠀⠀⠀⠀⠀⠀⢀⣼⠋⢸⡇⠀⠀⠀⠀⠀⠀⠀⣤⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⡄⠀⠀⠀⠙⢿⣿⣿⣿⣷⣤⡀⠀⠀⠀⠀⣠⣾⣿⣿⣿⣿⣿⣿⣿⠇⠀\\n⠀⢀⣠⣿⣿⣿⡿⠿⠿⠀⠀⠀⢀⣴⡿⠁⠀⣾⡇⠀⠀⠀⠀⠀⢀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠹⢿⣿⣿⣿⣿⣿⣶⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠀⠀\\n⣾⣿⣿⣿⣟⣿⣿⣷⣄⠀⢀⣴⣿⠟⠀⢀⣼⢟⣿⣀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀\\n⣿⣿⢛⣻⣝⡟⢿⣟⢿⣷⣨⡿⠃⠀⢠⣿⡋⢸⣿⠛⠻⢶⣄⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠒⠀⠀⠀⠀⠀⠀⠀⠈⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠏⠀⠀⠀\\n⡿⣽⣿⣿⣿⣿⣆⢻⡏⣿⡏⠀⠀⢀⣾⢋⣷⣾⣿⣦⡀⠀⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠿⣿⣿⣿⣿⣿⣿⣿⠟⠃⠀⠀⠀⣸\\n⡵⣿⣿⣿⣿⣿⣿⠄⣿⣿⡇⠀⣶⣿⣿⣸⣿⣿⣿⣿⣿⡆⠀⠀⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡇\\n⣧⡻⣿⣿⣿⣿⠟⣼⡏⣿⠇⠜⣰⣏⣿⣿⣿⣿⣿⣿⡏⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⡿⠀\\n⠈⠻⣦⣤⣤⣤⠞⢙⣹⡿⠀⣰⡿⠛⠿⢿⣿⣿⣿⡿⠀⠀⠀⠀⠀⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡿⠁⠀\\n⠀⠀⣠⣬⣭⡀⠀⢿⡏⠀⣴⣯⠁⠀⠒⠒⢾⣿⣿⡇⠀⣀⣀⡀⢠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣤⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣤⣾⠟⠃⠀⠀\\n⣤⣿⣿⣿⣷⣔⣖⣼⠀⣰⣿⢹⢀⣄⠀⠀⢸⣽⣿⡇⠀⠀⠀⢨⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣶⣶⣶⣶⣶⣿⣿⣿⠟⠋⠁⠀⠀⠀⠀\\n⢿⠸⣿⣿⣿⡿⣽⡿⣰⣿⢃⣿⠮⣽⣶⣴⡿⠿⠟⢧⡀⠀⠀⣿⠋⠉⠉⠙⠛⠛⠿⠿⠿⠿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⠋⠀⠀⠀⠀⠀⠀⠀⠀\\n⣧⡷⣌⣉⣉⣥⣿⣵⡏⣤⣿⠇⠀⠀⠀⠀⠀⠀⠀⠈⢿⡷⠾⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠳⠦⣤⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\\n⣿⣿⣶⣤⣤⣴⣿⣿⡿⠛⠁⠀⠀⠀⠀⠀⠀⠀⣀⢀⣋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠛⠶⢤⣤⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣤⣤⣤⣤⣤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣤⣴⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀ you can do all of this stuff on cpu or gpu if you have one⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⣿⠿⠋⠉⠁⠀⠀⠀⠹⢿⡿⣿⡻⠟⠿⣧⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣰⣿⣿⡿⡃⠀⠀⠀⠀⠀⠀⠀⠰⡄⢹⣿⡇⠀⢻⡿⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣿⣿⣿⣻⠁⠀⠀⠀⠀⠀⠀⠀⠀⣛⣼⡿⠃⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣤⣶⣿⣿⣷⣶⣦⣤⣀⡀⠀⠀                  running tensors on gpus\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠸⣿⡇⠻⣿⣤⣀⠀⠀⢀⣠⣤⡶⠟⠛⠁⠀⠀⠀⠈⠙⠓⠲⠶⠦⣤⣤⣄⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣼⠟⠋⢙⣿⣿⣿⣿⣿⣿⣿⣿⣶⣄               lets the computer do all the\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠹⣿⣦⠈⠉⠛⠛⠛⠋⠉⠀⠀⢀⣀⣀⣀⣠⣤⣤⣤⣤⣤⣤⣀⣀⣀⣀⠈⠉⠙⠓⠲⠶⢤⣀⡀⠀⠀⠀⡏⠀⢠⣾⠋⠁⠛⣉⡈⠙⠻⢿⣿⣿              calculations way more better\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠙⣿⠇⠀⢀⣀⣴⣶⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣶⣦⣄⣀⠀⠉⠉⠛⠛⠃⠀⠀⣿⡄⠀⠀⠀⠀⠀⠀⠀⠙⢿               so that everything is done\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⡾⢃⣤⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣄⡀⠀⠀⠀⠉⢷⣄⠀⠀⠀⠀⠀⠀⠀⢠               faster and more efficient\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⣾⣯⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⡀⠀⠀⠙⢷⣄⠀⠀⠀⠀⠀⠀\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⣿⣿⣿⣿⣿⣿⣿⠟⠋⠉⠉⠉⠉⠛⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⠀⠀⠙⠳⣤⣀⠀⠀⣐\n",
    "⠀⠀⠀⠀⠀⠀⠀⠀⣰⢟⣿⣿⣿⣿⣿⣿⡟⠁⠀⠀⠀⠀⠀⠀⠀⠀⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⣄⠀⠀⠈⠉⠛⠛⠉\n",
    "⠀⠀⠀⠀⠀⠀⠀⣼⢏⣾⣿⣿⣿⣿⣿⡟⠛⠛⣶⣄⣀⠀⠀⠀⠀⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⡀⠀⠀⢈⣼⡿\n",
    "⠀⠀⠀⠀⠀⠀⣸⠏⣾⣿⣿⣿⣿⣿⣿⠀⠀⢰⡏⠉⠙⠻⠷⣶⣤⣄⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠛⠋⠉⠛⠻⣿⣿⣿⣿⣿⣄⠀⢿⠁⠀\n",
    "⠀⠀⠀⠀⠀⣴⡏⣸⣿⣿⣿⣿⣿⣿⡏⠀⠀⢸⡇⠀⠀⠀⣠⡿⠃⠈⣽⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠃⠀⠀⠀⠀⠀⠀⠈⢻⣿⣿⣿⣿⣆⠸⣇⠀\n",
    "⠀⠀⠀⠀⣰⡟⢰⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠘⢷⣤⣤⠾⠋⠀⠀⣰⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀⠀⠀⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⡆⢹⡀\n",
    "⠀⠀⠀⣰⣿⠀⢸⣿⣿⣿⣿⣿⣿⣿⣿⣆⠀⠀⠀⠀⠀⠀⠀⣀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠷⠤⣤⣤⣤⣤⣤⣤⣤⣀⣀⣸⣿⣿⣿⣿⣷⡈⢧\n",
    "⠀⠀⠀⣿⡇⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣤⣤⣤⣴⣶⣿⣿⣿⣿⣿⣿⣿⠿⠿⠿⠿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠀⠀⢹⡆⠀⠀⠀⢈⣽⠃⠀⢹⣿⣿⣿⣿⣿⡇⠈\n",
    "⠀⠀⠀⣿⡇⠀⠈⢿⣿⣿⣿⣿⠿⠿⢿⣽⡟⣿⣿⣿⣿⣿⣿⡿⠟⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠉⠛⠻⢿⣿⣿⣿⣿⣿⣿⣿⠀⠀⠈⢷⡄⢀⣠⡿⠁⠀⢀⣿⣿⣿⣿⣿⣿⡇⠀                 \n",
    "⠀⠀⠀⣿⢑⣂⣠⣤⣿⣿⡷⢶⣶⣶⠀⠙⣿⢻⣿⡿⠟⠉⠁⠀⠀⠀⠀⢀⣤⣴⣶⣶⣶⣦⣤⣀⠀⠀⠀⠈⠛⢿⣿⣿⣿⣿⣧⠀⠀⠀⠙⠛⠉⠀⠀⢀⣼⣿⣿⣿⣿⣿⣿⡇⠀\n",
    "⠀⠀⠀⣿⣾⣿⣿⣏⠀⠀⠀⠀⠀⠀⢀⣼⠋⢸⡇⠀⠀⠀⠀⠀⠀⠀⣤⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⡄⠀⠀⠀⠙⢿⣿⣿⣿⣷⣤⡀⠀⠀⠀⠀⣠⣾⣿⣿⣿⣿⣿⣿⣿⠇⠀\n",
    "⠀⢀⣠⣿⣿⣿⡿⠿⠿⠀⠀⠀⢀⣴⡿⠁⠀⣾⡇⠀⠀⠀⠀⠀⢀⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡇⠀⠀⠀⠀⠀⠹⢿⣿⣿⣿⣿⣿⣶⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠀⠀\n",
    "⣾⣿⣿⣿⣟⣿⣿⣷⣄⠀⢀⣴⣿⠟⠀⢀⣼⢟⣿⣀⠀⠀⠀⠀⣸⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣷⠀⠀⠀⠀⠀⠀⠀⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠃⠀⠀\n",
    "⣿⣿⢛⣻⣝⡟⢿⣟⢿⣷⣨⡿⠃⠀⢠⣿⡋⢸⣿⠛⠻⢶⣄⠀⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡟⠒⠀⠀⠀⠀⠀⠀⠀⠈⠻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠏⠀⠀⠀\n",
    "⡿⣽⣿⣿⣿⣿⣆⢻⡏⣿⡏⠀⠀⢀⣾⢋⣷⣾⣿⣦⡀⠀⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⡿⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠿⣿⣿⣿⣿⣿⣿⣿⠟⠃⠀⠀⠀⣸\n",
    "⡵⣿⣿⣿⣿⣿⣿⠄⣿⣿⡇⠀⣶⣿⣿⣸⣿⣿⣿⣿⣿⡆⠀⠀⠙⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢠⡇\n",
    "⣧⡻⣿⣿⣿⣿⠟⣼⡏⣿⠇⠜⣰⣏⣿⣿⣿⣿⣿⣿⡏⠀⠀⠀⠀⠀⢻⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣇⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⡿⠀\n",
    "⠈⠻⣦⣤⣤⣤⠞⢙⣹⡿⠀⣰⡿⠛⠿⢿⣿⣿⣿⡿⠀⠀⠀⠀⠀⠀⣼⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣦⣄⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⡿⠁⠀\n",
    "⠀⠀⣠⣬⣭⡀⠀⢿⡏⠀⣴⣯⠁⠀⠒⠒⢾⣿⣿⡇⠀⣀⣀⡀⢠⣾⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣤⣤⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣠⣤⣾⠟⠃⠀⠀\n",
    "⣤⣿⣿⣿⣷⣔⣖⣼⠀⣰⣿⢹⢀⣄⠀⠀⢸⣽⣿⡇⠀⠀⠀⢨⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣶⣶⣶⣶⣶⣶⣿⣿⣿⠟⠋⠁⠀⠀⠀⠀\n",
    "⢿⠸⣿⣿⣿⡿⣽⡿⣰⣿⢃⣿⠮⣽⣶⣴⡿⠿⠟⢧⡀⠀⠀⣿⠋⠉⠉⠙⠛⠛⠿⠿⠿⠿⢿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⣿⠟⠋⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⣧⡷⣌⣉⣉⣥⣿⣵⡏⣤⣿⠇⠀⠀⠀⠀⠀⠀⠀⠈⢿⡷⠾⠋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠛⠳⠦⣤⣀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\n",
    "⣿⣿⣶⣤⣤⣴⣿⣿⡿⠛⠁⠀⠀⠀⠀⠀⠀⠀⣀⢀⣋⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠈⠉⠛⠶⢤⣤⣀⣀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀\"\"\" #give me a gpu or else (i will cry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
